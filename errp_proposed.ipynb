{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Written and Copyright by Mohit Agarwal\n",
    "#### Georgia Institute of Technology\n",
    "#### Email: magarwal37@gatech.edu\n",
    "\n",
    "### Code to decode Error-Potentials for the data recording on any game\n",
    "### Data should be present in the format of SXX-MMDD-XX.csv, _labels, and _metadata\n",
    "### 16 Electrode Channels: desribed in metadata file e.g. ['Pz', 'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'Cz', 'Fz', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fpz']\n",
    "\n",
    "### If want to classify data for every subject individually - put separate folder for every subject\n",
    "### If training and testing on different files, the folder structure should be similar in both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "##Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bisect\n",
    "import os\n",
    "import math\n",
    "from scipy.signal import *\n",
    "from pylab import *\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from random import shuffle\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import mne\n",
    "from mne.time_frequency import psd_array_multitaper, psd_multitaper\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyriemann\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "mne.set_log_level('ERROR')\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixed Parameters of the Algorithm\n",
    "data_dir = 'data'\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "filetype = 'openvibe'\n",
    "channels = ['Pz', 'F4', 'C4', 'P4', 'O2', 'F8', 'Fp2', 'Cz', 'Fz', 'F3', 'C3', 'P3', 'O1', 'F7', 'Fp1', 'Fpz']\n",
    "\n",
    "stim_nonErrp = 1\n",
    "stim_Errp = 0\n",
    "stim_dontknow = -1\n",
    "\n",
    "th_1 = 75 if filetype=='openvibe' else 150 # in uV : for adjacent spikes\n",
    "th_2 = 150 if filetype=='openvibe' else 300 # in uV : for min-max\n",
    "freq = 125.0\n",
    "time_delay = 0.0 # in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters of the algorithm [to change]\n",
    "\n",
    "# Misc\n",
    "flag_test = False # True if testing o.w. Cross-validate\n",
    "plt_err_dist = False  # Plots Error Distribution in time if set True\n",
    "selected_channels = None#[0,1,2,3,7,8,9,10,11,15] # If empty means all channels are selected, other option: [0,4,5]\n",
    "sync_method = \"event_date\" # options are \"default\" and \"event_date\"\n",
    "balanced_sampling = \"oversample\"\n",
    "\n",
    "\n",
    "\n",
    "# Plot-based\n",
    "plot_GAERP_allchan = False\n",
    "plot_GAERP_onechan = False\n",
    "plot_ERP_onechan = False\n",
    "chan_toPlot = 4\n",
    "\n",
    "total_channels = 16 if selected_channels is None else len(selected_channels) \n",
    "if selected_channels:\n",
    "    channels = [channels[i] for i in selected_channels]\n",
    "\n",
    "prob_th = 0.7\n",
    "num_sims = 10 # only used for cross-fold validation\n",
    "\n",
    "# Classification based overall\n",
    "low_freq = 1.0\n",
    "baseline_epoc = int(0.2*freq)\n",
    "epoc_window = int(0.8*freq)\n",
    "butter_filt_order = 4\n",
    "cv_folds = 10\n",
    "mean_baseline = False\n",
    "mean_per_chan = False\n",
    "car_pre = False #True\n",
    "high_freq = 15.0\n",
    "car_post = False\n",
    "\n",
    "\n",
    "xdawn_filters_spatial = 4\n",
    "xdawn_filters_time = 4\n",
    "xdawn_filters_freq = 4\n",
    "mean_baseline_spatial = True\n",
    "mean_baseline_time = True\n",
    "mean_baseline_freq = False\n",
    "mean_baseline_freq_pre = True\n",
    "nbins = 16\n",
    "\n",
    "kernel_freq, kernel_time = 'linear', 'linear'\n",
    "C_freq, C_time = 1.0, 1.0\n",
    "tmin_freq, tmax_freq = 0.4, 1.0\n",
    "fmin_freq, fmax_freq = 1, 14\n",
    "\n",
    "all_models = ['ensemble','spatial','freq','time']\n",
    "classifier = 'ensemble' # other options - 'spatial', 'freq', 'time', 'ensemble'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "# function to convert missing values to 0\n",
    "convert = lambda x: float(x.strip() or float('NaN'))\n",
    "\n",
    "def convert_codes(x):\n",
    "    if ':' in x:\n",
    "        return x.split(':',1)[1]\n",
    "    else:\n",
    "        return (x.strip() or float('NaN'))\n",
    "    \n",
    "def convert_time(t):\n",
    "    if '->' in t:\n",
    "        t, _ = t.split('->')\n",
    "        \n",
    "    (t1, ms) = t.split('.')\n",
    "    (h, m, s) = t1.split(':')\n",
    "    return (int(h) * 3600 + int(m) * 60 + int(s)) + 0.001*int(ms)\n",
    "\n",
    "# function to convert stimulations\n",
    "def to_byte(value, length):\n",
    "    for x in range(length):\n",
    "        yield value%256\n",
    "        value//=256\n",
    "\n",
    "# function to bandpass filter\n",
    "def bandpass(sig,band,fs,butter_filt_order):\n",
    "    B,A = butter(butter_filt_order, np.array(band)/(fs/2), btype='bandpass')\n",
    "    return lfilter(B, A, sig, axis=0)\n",
    "\n",
    "# Function to initialize confusion matrices\n",
    "def init_confusion_matrix():\n",
    "    cmat_train, cmat_test, auc_test = {}, {}, {}\n",
    "    if classifier=='ensemble':\n",
    "        for model in all_models:\n",
    "            cmat_train[model], cmat_test[model], auc_test[model] = np.zeros([3,3]), np.zeros([3,3]), []\n",
    "    else:\n",
    "        cmat_train[classifier], cmat_test[classifier], auc_test[classifier] = np.zeros([3,3]), np.zeros([3,3]), []\n",
    "        \n",
    "    return cmat_train, cmat_test, auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(DataFolder, label_file):\n",
    "    raw_file = label_file.split('_')[0] + '.csv'\n",
    "    if filetype=='openvibe':\n",
    "        raw_EEG = np.loadtxt(open(os.path.join(DataFolder,raw_file), \"rb\"), delimiter=\",\", skiprows=1, usecols=(0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17))\n",
    "    elif filetype=='openbci':\n",
    "        raw_EEG = np.loadtxt(open(os.path.join(DataFolder,raw_file), \"rb\"), delimiter=\",\", skiprows=6, usecols=(22,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16), converters={22: convert_time})\n",
    "        exp_start_time = raw_EEG[0,0]\n",
    "        raw_EEG[:,0] = raw_EEG[:,0] - exp_start_time\n",
    "    stim = pd.read_csv(os.path.join(DataFolder,label_file))\n",
    "    return raw_EEG, stim\n",
    "\n",
    "def pre_process(raw_EEG):\n",
    "    # Step 1: Subtract Mean per electrode\n",
    "    sig = raw_EEG[:,1:]\n",
    "    sig = sig[:,selected_channels] if selected_channels is not None else sig\n",
    "    if mean_per_chan:\n",
    "        sig_mean = np.mean(sig, axis=0)\n",
    "        sig = sig - sig_mean\n",
    "        \n",
    "    # Step 2: Bandpass in the given frequency range\n",
    "    sigF = bandpass(sig, [low_freq,high_freq], freq, butter_filt_order)\n",
    "    \n",
    "    # Step 3: Removing Average of all channels\n",
    "    if car_pre:\n",
    "        sig_mean = np.mean(sigF, axis=1)\n",
    "        sigF = sigF - np.reshape(sig_mean, [len(sig_mean),1])*np.ones([1,total_channels])\n",
    "    \n",
    "    return sigF\n",
    "\n",
    "def post_process(X):\n",
    "    if car_post:\n",
    "        temp =  np.repeat(np.mean(X,axis=2),X.shape[2],axis=1)\n",
    "        temp = np.reshape(temp, X.shape)\n",
    "        X = X - temp\n",
    "    X = X.transpose((0,2,1))\n",
    "    return X\n",
    "    \n",
    "\n",
    "# Arrange EEG data for training/testing\n",
    "def arrange_data(EEG, sigF, stim):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for stim_id, stim_code in enumerate(stim['label']):\n",
    "        if (not math.isnan(stim_code)) and stim_code!=33552 and stim_code!=33553 and stim_code!=33554:\n",
    "            if stim_code==stim_nonErrp or stim_code==stim_Errp:\n",
    "                if sync_method == \"default\":\n",
    "                    time_instant = stim['time1'][stim_id] + time_delay\n",
    "                else:\n",
    "                    time_instant = stim['time2'][stim_id] + time_delay\n",
    "                time_instant = time_instant\n",
    "                idx = bisect.bisect(EEG[:,0],time_instant)\n",
    "                X_temp = sigF[idx-1-baseline_epoc:idx+epoc_window,:]\n",
    "                if mean_baseline:\n",
    "                    X_mean = np.mean(sigF[idx-baseline_epoc:idx,:],0)\n",
    "                    X_temp = X_temp - X_mean\n",
    "                check = False\n",
    "                # Removing nan values and corrupted data\n",
    "                if np.isnan(X_temp).any():\n",
    "                    check = True\n",
    "                for i in range(total_channels):  \n",
    "                    X_diff = [abs(t - s) for s, t in zip(X_temp[:,i], X_temp[1:,i])]\n",
    "                    if max(X_diff)>th_1 or max(X_temp[:,i])-min(X_temp[:,i])>th_2:\n",
    "                        check = True\n",
    "                if not check:\n",
    "                    X.append(X_temp)\n",
    "                    Y.append(stim_code)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing Data from Train/Test Directory\n",
    "\n",
    "def dir_list(folder_path):\n",
    "    dir_list = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    dir_list.append('')\n",
    "    return dir_list\n",
    "\n",
    "data_dict = {}\n",
    "train_folder = os.path.join(data_dir,train_dir)\n",
    "test_folder = os.path.join(data_dir,test_dir)\n",
    "list_of_dir = dir_list(train_folder)\n",
    "if flag_test:\n",
    "    assert list_of_dir==dir_list(test_folder), \"Error: Test directory does not have same structure as train\"\n",
    "total_dirs = len(list_of_dir)\n",
    "for dir_idx in range(total_dirs):\n",
    "    paths = [[\"train\", os.path.join(train_folder, list_of_dir[dir_idx])], [\"test\", os.path.join(test_folder, list_of_dir[dir_idx])]]\n",
    "    if not flag_test:\n",
    "        paths.pop(1)\n",
    "    for items in paths:\n",
    "        train_test = items[0]\n",
    "        folder = items[1]\n",
    "        list_of_files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and '_labels' in f]\n",
    "#         print list_of_files\n",
    "        for curr_file in list_of_files:\n",
    "            \n",
    "            # Step 1: Load Raw EEG data (with filtered stimulations)\n",
    "            EEG, stim = loadData(folder, curr_file)\n",
    "            \n",
    "            # Step 2: Pre-process the raw EEG data before cutting in time-windows\n",
    "            sig = pre_process(EEG)\n",
    "            \n",
    "            # Step 3: Arrange Data\n",
    "            X_curr, Y_curr = arrange_data(EEG, sig, stim)\n",
    "            \n",
    "            # Making sure either the stimulations are present or raw data during stimulation is not discarded\n",
    "            if Y_curr.shape[0] > 0:\n",
    "                # Step 4: Post-process the Signals\n",
    "                X_curr = post_process(X_curr)\n",
    "\n",
    "                # Step 5: Arrange data in dictionary\n",
    "                key = 'data_' + list_of_dir[dir_idx] + '$' + train_test\n",
    "                if key in data_dict:\n",
    "                    [X, Y] = data_dict[key]\n",
    "                    X = np.concatenate((X,X_curr),axis=0)\n",
    "                    Y = np.concatenate((Y,Y_curr),axis=0)\n",
    "                    data_dict[key] = [X, Y]\n",
    "                else:\n",
    "                    data_dict[key] = [X_curr, Y_curr]\n",
    "                \n",
    "            \n",
    "exps = set()\n",
    "for key in data_dict.keys():\n",
    "    exps.add(key.split(\"$\")[0])\n",
    "\n",
    "# elements in the 'exps' set holds the folder names\n",
    "# [X_train, Y_train] = data_dict[exp+\"$train\"] loads the X_train and Y_train for training samples\n",
    "# All samples in all files inside one folder are concatenated when storing in data_dict\n",
    "\n",
    "# Size of X = [#of stim x #chan x # of timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Classification Pipelines after pre-processing\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def pipeline_spatial(X_train, X_test, Y_train):\n",
    "    \n",
    "    X_train_baseline_mean = np.mean(X_train[:,:,:baseline_epoc],2)\n",
    "    X_test_baseline_mean = np.mean(X_test[:,:,:baseline_epoc],2)\n",
    "\n",
    "    X_train = X_train[:,:,baseline_epoc:]\n",
    "    X_test = X_test[:,:,baseline_epoc:]\n",
    "    if mean_baseline_spatial:\n",
    "        X_train = X_train - np.repeat(X_train_baseline_mean[:,:,np.newaxis], X_train.shape[2], axis=2)\n",
    "        X_test = X_test -  np.repeat(X_test_baseline_mean[:,:,np.newaxis], X_train.shape[2], axis=2)\n",
    "    \n",
    "    filt_1 = pyriemann.estimation.XdawnCovariances(nfilter=xdawn_filters_spatial, estimator='lwf', xdawn_estimator='lwf')\n",
    "    filt_2 = pyriemann.channelselection.ElectrodeSelection(nelec=8, metric='riemann')\n",
    "    filt_3 = pyriemann.tangentspace.TangentSpace(metric='riemann', tsupdate=False) # 36-sized vector for nelec=8\n",
    "    filt_4 = Normalizer(norm='l1')                       \n",
    "    filt_5 = ElasticNet(l1_ratio=0.05, alpha=0.02, normalize=True)\n",
    "    filt_5 = SGDClassifier(loss='squared_hinge', class_weight='balanced', penalty='elasticnet', l1_ratio=0.5, alpha=0.002, max_iter=1000, tol=0.001)\n",
    "    \n",
    "    clf = make_pipeline(filt_1, filt_2, filt_3, filt_4)\n",
    "#     clf = make_pipeline(filt_1, filt_2, filt_3, filt_4, filt_5)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    train_features = clf.transform(X_train)\n",
    "    test_features = clf.transform(X_test)\n",
    "\n",
    "#     train_scores = clf.predict(X_train)\n",
    "#     test_scores = clf.predict(X_test)\n",
    "\n",
    "    \n",
    "#     thresholds = [x/100.0 for x in range(20,81,1)]\n",
    "#     acc_tr = []\n",
    "#     for th in thresholds:\n",
    "#         acc_tr.append([np.mean((train_scores > th)==Y_train),th])\n",
    "#     decision_th = max(acc_tr)[1]\n",
    "#     max_val = max(train_scores) - decision_th\n",
    "#     min_val = decision_th - min(train_scores)\n",
    "#     train_proba, test_proba = [], []\n",
    "    \n",
    "#     for item in train_scores:\n",
    "#         prob = [None, None]\n",
    "#         if item < decision_th:\n",
    "#             prob[0] = min(1, (decision_th - item)/(min_val))\n",
    "#             prob[1] = 1-prob[0]\n",
    "#         else:\n",
    "#             prob[1] = min(1, (item - decision_th)/(max_val))\n",
    "#             prob[0] = 1-prob[1]\n",
    "#         train_proba.append(prob)\n",
    "#     for item in test_scores:\n",
    "#         prob = [None, None]\n",
    "#         if item < decision_th:\n",
    "#             prob[0] = min(1, (decision_th - item)/(min_val))\n",
    "#             prob[1] = 1-prob[0]\n",
    "#         else:\n",
    "#             prob[1] = min(1, (item - decision_th)/(max_val))\n",
    "#             prob[0] = 1-prob[1]\n",
    "#         test_proba.append(prob)\n",
    "    \n",
    "    \n",
    "    clf_isotonic = CalibratedClassifierCV(filt_5, cv=2, method='isotonic')\n",
    "    clf_isotonic.fit(train_features, Y_train)\n",
    "    train_scores = clf_isotonic.predict_proba(train_features)\n",
    "    test_scores = clf_isotonic.predict_proba(test_features)\n",
    "    \n",
    "#     print(train_scores.shape, test_scores.shape)\n",
    "                                                           \n",
    "    return train_scores, test_scores #np.array(train_proba), np.array(test_proba)\n",
    "\n",
    "def pipeline_time(X_train, X_test, Y_train):\n",
    "    \n",
    "    X_train_baseline_mean = np.mean(X_train[:,:,:baseline_epoc],2)\n",
    "    X_test_baseline_mean = np.mean(X_test[:,:,:baseline_epoc],2)\n",
    "    \n",
    "#     B,A = butter(butter_filt_order, np.array([1.0, 10.0])/(freq/2), btype='bandpass')\n",
    "    \n",
    "#     X_train = lfilter(B, A, X_train, axis=2)\n",
    "#     X_test = lfilter(B, A, X_test, axis=2)\n",
    "\n",
    "    X_train = X_train[:,:,baseline_epoc:]\n",
    "    X_test = X_test[:,:,baseline_epoc:]\n",
    "    if mean_baseline_time:\n",
    "#         X_train_baseline_mean = np.reshape(X_train_baseline_mean, [,1]\n",
    "        X_train = X_train - np.repeat(X_train_baseline_mean[:,:,np.newaxis], X_train.shape[2], axis=2)\n",
    "        X_test = X_test -  np.repeat(X_test_baseline_mean[:,:,np.newaxis], X_train.shape[2], axis=2)\n",
    "        \n",
    "    filt_1 = pyriemann.spatialfilters.Xdawn(nfilter=xdawn_filters_time, estimator='lwf')\n",
    "    X_train = filt_1.fit_transform(X_train, Y_train)\n",
    "    X_test = filt_1.transform(X_test)\n",
    "    \n",
    "    t_idxs = int(epoc_window/nbins)\n",
    "    X_train_time = np.zeros([X_train.shape[0], xdawn_filters_time*2, nbins]) \n",
    "    for idd in range(X_train.shape[0]):\n",
    "        for i_bin in range(nbins):\n",
    "            t_range_start, t_range_end = i_bin*t_idxs, (i_bin+1)*t_idxs -1\n",
    "            X_train_time[idd,:,i_bin] = np.mean(X_train[idd,:,t_range_start:t_range_end], 1)\n",
    "            \n",
    "    X_train_time = X_train_time.reshape([X_train_time.shape[0], X_train_time.shape[1]*X_train_time.shape[2]])\n",
    "    \n",
    "    \n",
    "    X_test_time = np.zeros([X_test.shape[0], xdawn_filters_time*2, nbins]) \n",
    "    for idd in range(X_test.shape[0]):\n",
    "        for i_bin in range(nbins):\n",
    "            t_range_start, t_range_end = i_bin*t_idxs, (i_bin+1)*t_idxs -1\n",
    "            X_test_time[idd,:,i_bin] = np.mean(X_test[idd,:,t_range_start:t_range_end], 1)\n",
    "            \n",
    "    X_test_time = X_test_time.reshape([X_test_time.shape[0], X_test_time.shape[1]*X_test_time.shape[2]])\n",
    "    \n",
    "    filt_4 = Normalizer(norm='l2')  \n",
    "    filt_5 = svm.SVC(kernel=kernel_time, C=C_time, class_weight='balanced', gamma='auto', probability=True)\n",
    "    clf  = make_pipeline(filt_4, filt_5)\n",
    "    clf.fit(X_train_time, Y_train)\n",
    "    train_scores = clf.predict_proba(X_train_time)\n",
    "    \n",
    "    test_scores = clf.predict_proba(X_test_time)\n",
    "    return train_scores, test_scores\n",
    "            \n",
    "\n",
    "def pipeline_freq(X_train, X_test, Y_train):\n",
    "    \n",
    "    X_train_baseline_mean = np.mean(X_train[:,:,:baseline_epoc],2)\n",
    "    X_test_baseline_mean = np.mean(X_test[:,:,:baseline_epoc],2)\n",
    "    if mean_baseline_freq_pre:\n",
    "#         X_train_baseline_mean = np.reshape(X_train_baseline_mean, [,1]\n",
    "        X_train = X_train - np.repeat(X_train_baseline_mean[:,:,np.newaxis], X_train.shape[2], axis=2)\n",
    "        X_test = X_test -  np.repeat(X_test_baseline_mean[:,:,np.newaxis], X_train.shape[2], axis=2)\n",
    "        \n",
    "    filt_1 = pyriemann.spatialfilters.Xdawn(nfilter=xdawn_filters_time, estimator='lwf')\n",
    "    X_train = filt_1.fit_transform(X_train, Y_train)\n",
    "    X_test = filt_1.transform(X_test)\n",
    "    \n",
    "    filt_1 = pyriemann.spatialfilters.Xdawn(nfilter=xdawn_filters_freq, estimator='lwf')\n",
    "    X_train = filt_1.fit_transform(X_train, Y_train)\n",
    "    X_test = filt_1.transform(X_test)\n",
    "    info = mne.create_info(ch_names=2*xdawn_filters_freq*['a'], sfreq=freq, ch_types=2*xdawn_filters_freq*['eeg'])\n",
    "    \n",
    "    # 14 for 2-26, 0.6s long\n",
    "    # 7 for 1-12, 0.6s long\n",
    "    X_train_freq = np.zeros([X_train.shape[0], 2*xdawn_filters_freq, 8])    \n",
    "    for idd in range(X_train.shape[0]):\n",
    "        raw = mne.io.RawArray(X_train[idd,:,:], info)\n",
    "        psds, freqs = psd_multitaper(raw, low_bias=False, tmin=tmin_freq, tmax=tmax_freq, fmin=fmin_freq, fmax=fmax_freq, proj=False, n_jobs=8)\n",
    "        if mean_baseline_freq:\n",
    "            psds_base, freqs_base = psd_multitaper(raw, low_bias=False, tmin=0.0, tmax=0.2, fmin=fmin_freq, fmax=fmax_freq, proj=False, n_jobs=8)\n",
    "            temp = 10*np.log10(psds) - np.mean(psds_base)\n",
    "        else:\n",
    "            temp = 10*np.log10(psds)\n",
    "        X_train_freq[idd,:,:] = temp[:,:]\n",
    "        \n",
    "        \n",
    "    X_train_freq = X_train_freq.reshape([X_train_freq.shape[0], X_train_freq.shape[1]*X_train_freq.shape[2]])\n",
    "    \n",
    "    X_test_freq = np.zeros([X_test.shape[0], 2*xdawn_filters_freq, 8])    \n",
    "    for idd in range(X_test.shape[0]):\n",
    "        raw = mne.io.RawArray(X_test[idd,:,:], info)\n",
    "        psds, freqs = psd_multitaper(raw, low_bias=False, tmin=tmin_freq, tmax=tmax_freq, fmin=fmin_freq, fmax=fmax_freq, proj=False, n_jobs=8)\n",
    "        if mean_baseline_freq:\n",
    "            psds_base, freqs_base = psd_multitaper(raw, low_bias=False, tmin=0.0, tmax=0.2, fmin=fmin_freq, fmax=fmax_freq, proj=False, n_jobs=8)\n",
    "            temp = 10*np.log10(psds) - np.mean(psds_base)\n",
    "        else:\n",
    "            temp = 10*np.log10(psds)\n",
    "        X_test_freq[idd,:,:] = temp[:,:]\n",
    "    X_test_freq = X_test_freq.reshape([X_test_freq.shape[0], X_test_freq.shape[1]*X_test_freq.shape[2]])\n",
    "    \n",
    "#     filt_4 = Normalizer(norm='l2')  \n",
    "    filt_5 = svm.SVC(kernel=kernel_freq, C=C_freq, class_weight='balanced', gamma='auto', probability=True)\n",
    "    clf  = make_pipeline(filt_5)\n",
    "    clf.fit(X_train_freq, Y_train)\n",
    "    train_scores = clf.predict_proba(X_train_freq)\n",
    "    \n",
    "    test_scores = clf.predict_proba(X_test_freq)\n",
    "    return train_scores, test_scores\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(X_train, Y_train): \n",
    "    if balanced_sampling == \"undersample\":\n",
    "        rusU = RandomUnderSampler(return_indices=True)\n",
    "        X_new = np.reshape(X_train,[X_train.shape[0],X_train.shape[1]*X_train.shape[2]])\n",
    "        X_resU, Y_resU, id_rusU = rusU.fit_resample(X_new, Y_train)\n",
    "        X_new2 = np.reshape(X_resU,[X_resU.shape[0],X_train.shape[1],X_train.shape[2]])\n",
    "        X_sim = X_new2\n",
    "        Y_sim = Y_resU\n",
    "    elif balanced_sampling == \"oversample\":\n",
    "        rusO = RandomOverSampler(return_indices=True)\n",
    "        X_new = np.reshape(X_train,[X_train.shape[0],X_train.shape[1]*X_train.shape[2]])\n",
    "        X_resO, Y_resO, id_rusO = rusO.fit_resample(X_new, Y_train)\n",
    "        X_new2 = np.reshape(X_resO,[X_resO.shape[0],X_train.shape[1],X_train.shape[2]])\n",
    "        X_sim = X_new2\n",
    "        Y_sim = Y_resO\n",
    "    elif balanced_sampling == \"smote\":\n",
    "        rusS = SMOTE()\n",
    "        X_new = np.reshape(X_train,[X_train.shape[0],X_train.shape[1]*X_train.shape[2]])\n",
    "        X_resS, Y_resS = rusS.fit_resample(X_new, Y_train)\n",
    "        X_new2 = np.reshape(X_resS,[X_resS.shape[0],X_train.shape[1],X_train.shape[2]])\n",
    "        X_sim = X_new2\n",
    "        Y_sim = Y_resS\n",
    "        \n",
    "    return X_sim, Y_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acutal Prediction\n",
    "\n",
    "def actual_prediction(train_prob, test_prob):\n",
    "    train_preds, test_preds = np.zeros([train_prob.shape[0]]), np.zeros([test_prob.shape[0]])\n",
    "    for idx in range(train_preds.shape[0]):\n",
    "#         if train_prob[idx] >= 0.6:\n",
    "#             train_preds[idx] = 1\n",
    "#         else:\n",
    "#             train_preds[idx] = 0\n",
    "#     for idx in range(test_preds.shape[0]):\n",
    "#         if test_prob[idx] >= 0.55:\n",
    "#             test_preds[idx] = 1\n",
    "#         else:\n",
    "#             test_preds[idx] = 0\n",
    "        if train_prob[idx] >= prob_th:\n",
    "            train_preds[idx] = 1\n",
    "        elif train_prob[idx] < 1-prob_th:\n",
    "            train_preds[idx] = 0\n",
    "        else:\n",
    "            train_preds[idx] = -1\n",
    "    for idx in range(test_preds.shape[0]):\n",
    "        if test_prob[idx] >= prob_th:\n",
    "            test_preds[idx] = 1\n",
    "        elif test_prob[idx] < 1-prob_th:\n",
    "            test_preds[idx] = 0\n",
    "        else:\n",
    "            test_preds[idx] = -1\n",
    "    \n",
    "    return train_preds, test_preds\n",
    "\n",
    "def predict(X_train, X_test, Y_train, Y_test):\n",
    "    train_preds, test_preds = [], []\n",
    "    cmat_train, cmat_test, auc_test = dict(), dict(), dict()\n",
    "    \n",
    "    if classifier=='spatial':\n",
    "        train_spatial_prob, test_spatial_prob = pipeline_spatial(X_train, X_test, Y_train)\n",
    "        train_preds, test_preds = actual_prediction(train_spatial_prob[:,1], test_spatial_prob[:,1])\n",
    "        cmat_train['spatial'] = confusion_matrix(Y_train, train_preds, labels=[-1,0,1])\n",
    "        cmat_test['spatial'] = confusion_matrix(Y_test, test_preds, labels=[-1,0,1])  \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, test_spatial_prob, pos_label=1) #label=0 corresponds to errp\n",
    "        auc_test['spatial'] = metrics.auc(fpr,tpr)\n",
    "    elif classifier=='freq':\n",
    "        train_freq_prob, test_freq_prob = pipeline_freq(X_train, X_test, Y_train)\n",
    "        train_preds, test_preds = actual_prediction(train_freq_prob[:,1], test_freq_prob[:,1])\n",
    "        cmat_train['freq'] = confusion_matrix(Y_train, train_preds, labels=[-1,0,1])\n",
    "        cmat_test['freq'] = confusion_matrix(Y_test, test_preds, labels=[-1,0,1])  \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, test_freq_prob, pos_label=1) #label=0 corresponds to errp\n",
    "        auc_test['freq'] = metrics.auc(fpr,tpr)\n",
    "    elif classifier=='time':\n",
    "        train_time_prob, test_time_prob = pipeline_time(X_train, X_test, Y_train)\n",
    "        train_preds, test_preds = actual_prediction(train_time_prob[:,1], test_time_prob[:,1])\n",
    "        cmat_train['time'] = confusion_matrix(Y_train, train_preds, labels=[-1,0,1])\n",
    "        cmat_test['time'] = confusion_matrix(Y_test, test_preds, labels=[-1,0,1])  \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, test_time_prob, pos_label=1) #label=0 corresponds to errp\n",
    "        auc_test['time'] = metrics.auc(fpr,tpr)\n",
    "    elif classifier=='ensemble': # classifier is ensemble\n",
    "        train_spatial_prob, test_spatial_prob = pipeline_spatial(X_train, X_test, Y_train)\n",
    "        train_freq_prob, test_freq_prob = pipeline_freq(X_train, X_test, Y_train)\n",
    "        train_time_prob, test_time_prob = pipeline_time(X_train, X_test, Y_train)\n",
    "        \n",
    "        train_prob = (train_spatial_prob[:,1] + train_time_prob[:,1] + train_freq_prob[:,1])/3.0\n",
    "        test_prob = (test_spatial_prob[:,1] + test_time_prob[:,1] + test_freq_prob[:,1])/3.0\n",
    "        #Spatial\n",
    "        train_preds, test_preds = actual_prediction(train_spatial_prob[:,1], test_spatial_prob[:,1])\n",
    "        cmat_train['spatial'] = confusion_matrix(Y_train, train_preds, labels=[-1,0,1])\n",
    "        cmat_test['spatial'] = confusion_matrix(Y_test, test_preds, labels=[-1,0,1]) \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, test_spatial_prob[:,1], pos_label=1) #label=0 corresponds to errp\n",
    "        auc_test['spatial'] = metrics.auc(fpr,tpr)\n",
    "        #Freq\n",
    "        train_preds, test_preds = actual_prediction(train_freq_prob[:,1], test_freq_prob[:,1])\n",
    "        cmat_train['freq'] = confusion_matrix(Y_train, train_preds, labels=[-1,0,1])\n",
    "        cmat_test['freq'] = confusion_matrix(Y_test, test_preds, labels=[-1,0,1])  \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, test_freq_prob[:,1], pos_label=1) #label=0 corresponds to errp\n",
    "        auc_test['freq'] = metrics.auc(fpr,tpr)\n",
    "        #Time\n",
    "        train_preds, test_preds = actual_prediction(train_time_prob[:,1], test_time_prob[:,1])\n",
    "        cmat_train['time'] = confusion_matrix(Y_train, train_preds, labels=[-1,0,1])\n",
    "        cmat_test['time'] = confusion_matrix(Y_test, test_preds, labels=[-1,0,1])\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, test_time_prob[:,1], pos_label=1) #label=0 corresponds to errp\n",
    "        auc_test['time'] = metrics.auc(fpr,tpr)\n",
    "        #Ensemble\n",
    "        train_preds, test_preds = actual_prediction(train_prob, test_prob)\n",
    "        cmat_train['ensemble'] = confusion_matrix(Y_train, train_preds, labels=[-1,0,1])\n",
    "        cmat_test['ensemble'] = confusion_matrix(Y_test, test_preds, labels=[-1,0,1])\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, test_prob, pos_label=1) #label=0 corresponds to errp\n",
    "        auc_test['ensemble'] = metrics.auc(fpr,tpr)\n",
    "        \n",
    "#         for idx in range(test_preds.shape[0]):\n",
    "#             if not (test_preds[idx] == Y_test[idx]):\n",
    "#                 print test_preds[idx], Y_test[idx], test_spatial_prob[idx,1], test_freq_prob[idx,1], test_time_prob[idx,1], test_prob[idx]\n",
    "                \n",
    "        \n",
    "        \n",
    "\n",
    "    # here index 1 and value 1 - corresponds to the Non-ErrP\n",
    "    \n",
    "\n",
    "    return cmat_train, cmat_test, auc_test, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation: Returns train and test confusion matrix for X and Y\n",
    "\n",
    "def predict_CV(X, Y):    \n",
    "    kfold = KFold(n_splits=cv_folds, shuffle=True)\n",
    "    cmat_train, cmat_test, auc_test = init_confusion_matrix()\n",
    "        \n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "      \n",
    "        X_train, Y_train = balance_dataset(X_train, Y_train)\n",
    "        \n",
    "        cm_train, cm_test, auc_curr, _ = predict(X_train, X_test, Y_train, Y_test)\n",
    "        \n",
    "        if classifier=='ensemble':\n",
    "            for model in all_models:\n",
    "                cmat_train[model] = cmat_train[model] + cm_train[model]\n",
    "                cmat_test[model] = cmat_test[model] + cm_test[model]\n",
    "#                 auc_test[model] = auc_test[model] + auc_curr[model]\n",
    "                auc_test[model].append(auc_curr[model])\n",
    "        else:\n",
    "            cmat_train[classifier] = cmat_train[classifier] + cm_train[classifier]\n",
    "            cmat_test[classifier] = cmat_test[classifier] + cm_test[classifier]\n",
    "#             auc_test[classifier] = auc_test[classifier] + auc_curr[classifier]\n",
    "            auc_test[classifier].append(auc_curr[classifier])\n",
    "    \n",
    "    if classifier=='ensemble':\n",
    "        for model in all_models:\n",
    "            cmat_train[model] = cmat_train[model]*1.0/cv_folds\n",
    "            cmat_test[model] = cmat_test[model]*1.0/cv_folds\n",
    "#             auc_test[model] = auc_test[model]*1.0/cv_folds\n",
    "            auc_test[model] = mean(auc_test[model])\n",
    "    else:\n",
    "        cmat_train[classifier] = cmat_train[classifier]*1.0/cv_folds\n",
    "        cmat_test[classifier] = cmat_test[classifier]*1.0/cv_folds\n",
    "#         auc_test[classifier] = auc_test[classifier]*1.0/cv_folds\n",
    "        auc_test[classifier] = mean(auc_test[classifier])\n",
    "            \n",
    "    return cmat_train, cmat_test, auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 0/21\n",
      "Processing.. 1/21\n",
      "Processing.. 2/21\n",
      "Processing.. 3/21\n",
      "Processing.. 4/21\n",
      "Processing.. 5/21\n",
      "Processing.. 6/21\n",
      "Processing.. 7/21\n",
      "Processing.. 8/21\n",
      "Processing.. 9/21\n",
      "Processing.. 10/21\n",
      "Processing.. 11/21\n",
      "Processing.. 12/21\n",
      "Processing.. 13/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 14/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 15/21\n",
      "Processing.. 16/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 17/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 18/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:113: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 19/21\n",
      "Processing.. 20/21\n"
     ]
    }
   ],
   "source": [
    "# Computing Accuracies\n",
    "\n",
    "result_dict = {}\n",
    "    \n",
    "cmat_train, cmat_test, auc_test = init_confusion_matrix()\n",
    "\n",
    "#ToDO: FIX flag_test part of code\n",
    "for exp_idx, exp in enumerate(exps):\n",
    "    print(\"Processing.. \"+str(exp_idx)+\"/\"+str(len(exps))+\"\")\n",
    "    Y_pred = None\n",
    "    Y_test = None\n",
    "    if flag_test:\n",
    "        [X_train, Y_train] = data_dict[exp+\"$train\"]\n",
    "        index_shuffle = [i for i in range(X_train.shape[0])]\n",
    "        shuffle(index_shuffle)\n",
    "        X_train = X_train[index_shuffle,:,:]\n",
    "        Y_train = Y_train[index_shuffle]\n",
    "        [X_test, Y_test] = data_dict[exp+\"$test\"]\n",
    "        X_train, Y_train = balance_dataset(X_train, Y_train)\n",
    "        cmat_train, cmat_test, auc_test, Y_pred = predict(X_train, X_test, Y_train, Y_test)\n",
    "    else:\n",
    "        [X_train, Y_train] = data_dict[exp+\"$train\"]\n",
    "        \n",
    "        cmat_train, cmat_test, auc_test = init_confusion_matrix()\n",
    "    \n",
    "        for ind in range(num_sims):\n",
    "            cm_train, cm_test, auc_curr = predict_CV(X_train,Y_train)\n",
    "            \n",
    "            if classifier=='ensemble':\n",
    "                for model in all_models:\n",
    "                    cmat_train[model] = cmat_train[model] + cm_train[model]\n",
    "                    cmat_test[model] = cmat_test[model] + cm_test[model]\n",
    "#                     auc_test[model] = auc_test[model] + auc_curr[model]\n",
    "                    auc_test[model].append(auc_curr[model])\n",
    "            else:\n",
    "                cmat_train[classifier] = cmat_train[classifier] + cm_train[classifier]\n",
    "                cmat_test[classifier] = cmat_test[classifier] + cm_test[classifier]\n",
    "#                 auc_test[classifier] = auc_test[classifier] + auc_curr[classifier]\n",
    "                auc_test[classifier].append(auc_curr[classifier])\n",
    "        \n",
    "        for model in cmat_train.keys():\n",
    "            cmat_train[model] = cmat_train[model]/num_sims\n",
    "            cmat_test[model] = cmat_test[model]/num_sims\n",
    "#             auc_test[model] = auc_test[model]/num_sims\n",
    "            auc_test[model] = mean(auc_test[model])\n",
    "    \n",
    "    assert not exp in result_dict.keys()\n",
    "    train_errp_split = (sum(cmat_train[classifier],1)*1.0/sum(cmat_train[classifier]))[1]\n",
    "    test_errp_split = (sum(cmat_test[classifier],1)*1.0/sum(cmat_test[classifier]))[1]\n",
    "    result_dict[exp] = [cmat_train, cmat_test, auc_test, X_train.shape[0], train_errp_split, test_errp_split, Y_test, Y_pred]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTEST\u001b[0m\n",
      "Name, Avg Accuracy, ErrP Precision, AUC Score, F1, ErrP Acc, NonErrP Acc, Avg DN, ErrP DN, NonErrP DN\n",
      "\u001b[1mENSEMBLE\u001b[0m\n",
      "data_S15-0909, 0.881, 0.878, 0.909, 0.832, 0.790, 0.972, 0.154, 0.153, 0.155, \n",
      "data_S07-0729, 0.918, 0.947, 0.926, 0.893, 0.845, 0.990, 0.131, 0.125, 0.138, \n",
      "data_S01-0717, 0.771, 0.882, 0.824, 0.681, 0.555, 0.987, 0.132, 0.124, 0.140, \n",
      "data_S07-0718, 0.922, 0.946, 0.928, 0.905, 0.867, 0.976, 0.185, 0.190, 0.179, \n",
      "data_S02-0717, 0.900, 0.902, 0.838, 0.881, 0.861, 0.938, 0.443, 0.452, 0.435, \n",
      "data_S01-0719, 0.910, 0.902, 0.909, 0.886, 0.871, 0.949, 0.217, 0.226, 0.208, \n",
      "data_S16-0910, 0.921, 0.866, 0.940, 0.868, 0.869, 0.972, 0.121, 0.104, 0.139, \n",
      "data_S06-0801, 0.873, 0.822, 0.890, 0.799, 0.778, 0.969, 0.194, 0.174, 0.213, \n",
      "data_S01-0629, 0.885, 0.865, 0.902, 0.832, 0.801, 0.969, 0.181, 0.176, 0.185, \n",
      "data_S02-0716, 0.818, 0.797, 0.848, 0.730, 0.673, 0.963, 0.212, 0.211, 0.212, \n",
      "data_S03-0628, 0.750, 0.657, 0.774, 0.602, 0.556, 0.944, 0.345, 0.355, 0.335, \n",
      "data_S07-0626, 0.953, 0.956, 0.967, 0.935, 0.916, 0.989, 0.090, 0.095, 0.085, \n",
      "data_S02-0718, 0.791, 0.858, 0.841, 0.707, 0.602, 0.980, 0.145, 0.138, 0.152, \n",
      "data_S09-0627, 0.607, 0.758, nan, 0.345, 0.223, 0.990, 0.032, 0.026, 0.039, \n",
      "data_S05-0628, 0.758, 0.746, nan, 0.625, 0.538, 0.979, 0.133, 0.102, 0.164, \n",
      "data_S01-0724, 0.960, 0.947, 0.962, 0.941, 0.934, 0.986, 0.110, 0.102, 0.119, \n",
      "data_S08-0807, 0.720, 0.786, nan, 0.593, 0.476, 0.963, 0.261, 0.266, 0.257, \n",
      "data_S07-0716, 0.922, 0.967, nan, 0.903, 0.848, 0.996, 0.017, 0.020, 0.015, \n",
      "data_S12-0803, 0.788, 0.857, nan, 0.699, 0.590, 0.985, 0.033, 0.020, 0.046, \n",
      "data_S04-0628, 0.777, 0.690, 0.816, 0.642, 0.600, 0.953, 0.273, 0.264, 0.281, \n",
      "data_S09-0628, 0.872, 0.750, 0.886, 0.769, 0.789, 0.955, 0.149, 0.123, 0.175, \n",
      "\u001b[1mSPATIAL\u001b[0m\n",
      "data_S15-0909, 0.855, 0.714, 0.878, 0.758, 0.808, 0.901, 0.209, 0.226, 0.193, \n",
      "data_S07-0729, 0.902, 0.796, 0.919, 0.825, 0.856, 0.948, 0.187, 0.203, 0.171, \n",
      "data_S01-0717, 0.779, 0.615, 0.813, 0.630, 0.646, 0.912, 0.160, 0.161, 0.159, \n",
      "data_S07-0718, 0.874, 0.836, 0.895, 0.837, 0.839, 0.909, 0.207, 0.206, 0.209, \n",
      "data_S02-0717, 0.831, 0.777, 0.814, 0.798, 0.821, 0.842, 0.374, 0.386, 0.361, \n",
      "data_S01-0719, 0.872, 0.821, 0.887, 0.837, 0.853, 0.891, 0.201, 0.209, 0.193, \n",
      "data_S16-0910, 0.900, 0.746, 0.924, 0.806, 0.876, 0.924, 0.148, 0.151, 0.145, \n",
      "data_S06-0801, 0.832, 0.588, 0.849, 0.674, 0.789, 0.876, 0.218, 0.223, 0.212, \n",
      "data_S01-0629, 0.865, 0.731, 0.886, 0.769, 0.812, 0.919, 0.227, 0.251, 0.202, \n",
      "data_S02-0716, 0.787, 0.602, 0.806, 0.644, 0.692, 0.881, 0.244, 0.254, 0.235, \n",
      "data_S03-0628, 0.738, 0.518, 0.734, 0.557, 0.604, 0.872, 0.388, 0.411, 0.365, \n",
      "data_S07-0626, 0.931, 0.865, 0.956, 0.883, 0.902, 0.960, 0.110, 0.114, 0.106, \n",
      "data_S02-0718, 0.778, 0.637, 0.800, 0.643, 0.649, 0.908, 0.214, 0.228, 0.200, \n",
      "data_S09-0627, 0.698, 0.645, nan, 0.525, 0.442, 0.954, 0.075, 0.073, 0.077, \n",
      "data_S05-0628, 0.764, 0.545, nan, 0.580, 0.619, 0.908, 0.223, 0.233, 0.213, \n",
      "data_S01-0724, 0.912, 0.808, 0.943, 0.847, 0.890, 0.933, 0.128, 0.131, 0.125, \n",
      "data_S08-0807, 0.686, 0.592, nan, 0.536, 0.490, 0.883, 0.277, 0.286, 0.268, \n",
      "data_S07-0716, 0.920, 0.847, nan, 0.857, 0.866, 0.974, 0.049, 0.052, 0.045, \n",
      "data_S12-0803, 0.813, 0.668, nan, 0.677, 0.686, 0.941, 0.049, 0.037, 0.061, \n",
      "data_S04-0628, 0.786, 0.557, 0.800, 0.621, 0.701, 0.871, 0.270, 0.294, 0.246, \n",
      "data_S09-0628, 0.848, 0.598, 0.877, 0.692, 0.821, 0.875, 0.118, 0.110, 0.127, \n",
      "\u001b[1mFREQ\u001b[0m\n",
      "data_S15-0909, 0.757, 0.612, 0.774, 0.627, 0.644, 0.870, 0.335, 0.427, 0.244, \n",
      "data_S07-0729, 0.806, 0.614, 0.826, 0.660, 0.714, 0.898, 0.225, 0.283, 0.167, \n",
      "data_S01-0717, 0.666, 0.426, 0.707, 0.450, 0.476, 0.857, 0.339, 0.427, 0.251, \n",
      "data_S07-0718, 0.790, 0.778, 0.765, 0.741, 0.707, 0.873, 0.531, 0.558, 0.503, \n",
      "data_S02-0717, 0.704, 0.723, 0.655, 0.670, 0.625, 0.783, 0.786, 0.792, 0.780, \n",
      "data_S01-0719, 0.801, 0.759, 0.794, 0.750, 0.741, 0.860, 0.480, 0.521, 0.440, \n",
      "data_S16-0910, 0.773, 0.553, 0.801, 0.609, 0.678, 0.868, 0.252, 0.281, 0.223, \n",
      "data_S06-0801, 0.705, 0.451, 0.721, 0.501, 0.564, 0.846, 0.404, 0.483, 0.326, \n",
      "data_S01-0629, 0.788, 0.616, 0.812, 0.649, 0.685, 0.890, 0.256, 0.284, 0.228, \n",
      "data_S02-0716, 0.768, 0.603, 0.793, 0.629, 0.657, 0.879, 0.323, 0.407, 0.238, \n",
      "data_S03-0628, 0.672, 0.429, 0.675, 0.467, 0.514, 0.831, 0.449, 0.544, 0.354, \n",
      "data_S07-0626, 0.855, 0.764, 0.894, 0.771, 0.779, 0.931, 0.155, 0.211, 0.099, \n",
      "data_S02-0718, 0.739, 0.562, 0.792, 0.578, 0.594, 0.883, 0.306, 0.392, 0.220, \n",
      "data_S09-0627, 0.620, 0.370, nan, 0.368, 0.365, 0.875, 0.269, 0.349, 0.190, \n",
      "data_S05-0628, 0.612, 0.291, nan, 0.334, 0.391, 0.833, 0.418, 0.537, 0.300, \n",
      "data_S01-0724, 0.813, 0.660, 0.832, 0.697, 0.739, 0.887, 0.221, 0.227, 0.215, \n",
      "data_S08-0807, 0.550, 0.328, nan, 0.329, 0.330, 0.770, 0.770, 0.856, 0.685, \n",
      "data_S07-0716, 0.821, 0.646, nan, 0.675, 0.707, 0.936, 0.072, 0.096, 0.048, \n",
      "data_S12-0803, 0.695, 0.495, nan, 0.485, 0.474, 0.916, 0.040, 0.040, 0.041, \n",
      "data_S04-0628, 0.590, 0.327, 0.603, 0.358, 0.395, 0.785, 0.637, 0.746, 0.529, \n",
      "data_S09-0628, 0.668, 0.421, 0.674, 0.459, 0.504, 0.833, 0.448, 0.536, 0.361, \n",
      "\u001b[1mTIME\u001b[0m\n",
      "data_S15-0909, 0.839, 0.749, 0.891, 0.751, 0.752, 0.926, 0.078, 0.068, 0.088, \n",
      "data_S07-0729, 0.881, 0.772, 0.911, 0.792, 0.814, 0.947, 0.065, 0.055, 0.074, \n",
      "data_S01-0717, 0.726, 0.606, 0.771, 0.561, 0.523, 0.929, 0.098, 0.086, 0.110, \n",
      "data_S07-0718, 0.885, 0.864, 0.928, 0.853, 0.842, 0.928, 0.088, 0.092, 0.084, \n",
      "data_S02-0717, 0.801, 0.774, 0.798, 0.765, 0.755, 0.847, 0.264, 0.265, 0.263, \n",
      "data_S01-0719, 0.861, 0.823, 0.878, 0.824, 0.824, 0.898, 0.135, 0.130, 0.140, \n",
      "data_S16-0910, 0.858, 0.734, 0.913, 0.757, 0.781, 0.936, 0.051, 0.038, 0.064, \n",
      "data_S06-0801, 0.825, 0.679, 0.866, 0.701, 0.725, 0.926, 0.086, 0.070, 0.102, \n",
      "data_S01-0629, 0.814, 0.675, 0.860, 0.696, 0.720, 0.909, 0.110, 0.099, 0.122, \n",
      "data_S02-0716, 0.692, 0.525, 0.737, 0.512, 0.499, 0.885, 0.153, 0.138, 0.169, \n",
      "data_S03-0628, 0.706, 0.502, 0.718, 0.522, 0.544, 0.868, 0.197, 0.173, 0.222, \n",
      "data_S07-0626, 0.888, 0.796, 0.930, 0.813, 0.832, 0.944, 0.062, 0.052, 0.071, \n",
      "data_S02-0718, 0.684, 0.554, 0.754, 0.500, 0.456, 0.912, 0.106, 0.088, 0.124, \n",
      "data_S09-0627, 0.605, 0.531, nan, 0.344, 0.254, 0.955, 0.032, 0.022, 0.041, \n",
      "data_S05-0628, 0.792, 0.633, nan, 0.638, 0.643, 0.940, 0.054, 0.041, 0.066, \n",
      "data_S01-0724, 0.920, 0.870, 0.952, 0.875, 0.880, 0.960, 0.049, 0.040, 0.057, \n",
      "data_S08-0807, 0.738, 0.677, nan, 0.625, 0.580, 0.896, 0.119, 0.106, 0.131, \n",
      "data_S07-0716, 0.903, 0.913, nan, 0.864, 0.819, 0.988, 0.011, 0.011, 0.011, \n",
      "data_S12-0803, 0.769, 0.706, nan, 0.633, 0.574, 0.964, 0.020, 0.014, 0.026, \n",
      "data_S04-0628, 0.735, 0.551, 0.800, 0.566, 0.582, 0.889, 0.133, 0.124, 0.143, \n",
      "data_S09-0628, 0.813, 0.667, 0.887, 0.683, 0.699, 0.927, 0.068, 0.056, 0.079, \n",
      "\u001b[91mTRAIN\u001b[0m\n",
      "\u001b[1mENSEMBLE\u001b[0m\n",
      "Name             #Samples      Tr_split      Te_split      Train ErrP Prec      Train ErrP Acc      Train Non_ErrP Acc\n",
      "-------------  ------------  ------------  ------------  -------------------  ------------------  ----------------------\n",
      "data_S15-0909           324           0.5         0.241                0.992               0.992                   0.991\n",
      "data_S07-0729           461           0.5         0.191                0.997               0.982                   0.997\n",
      "data_S01-0717           218           0.5         0.188                0.998               0.995                   0.998\n",
      "data_S07-0718           290           0.5         0.355                0.995               0.991                   0.995\n",
      "data_S02-0717           254           0.5         0.425                0.992               0.984                   0.993\n",
      "data_S01-0719           296           0.5         0.375                0.985               0.992                   0.985\n",
      "data_S16-0910           409           0.5         0.205                0.989               0.995                   0.988\n",
      "data_S06-0801           400           0.5         0.198                0.99                0.987                   0.989\n",
      "data_S01-0629           375           0.5         0.216                0.994               0.995                   0.994\n",
      "data_S02-0716           290           0.5         0.228                0.99                0.995                   0.989\n",
      "data_S03-0628           400           0.5         0.2                  0.977               0.983                   0.977\n",
      "data_S07-0626           451           0.5         0.222                0.995               0.996                   0.995\n",
      "data_S02-0718           220           0.5         0.218                0.999               0.996                   0.999\n",
      "data_S09-0627           113           0.5         0.177                1                   1                       1\n",
      "data_S05-0628           261           0.5         0.153                0.997               0.995                   0.997\n",
      "data_S01-0724           488           0.5         0.242                0.994               0.994                   0.994\n",
      "data_S08-0807           195           0.5         0.282                0.998               0.986                   0.998\n",
      "data_S07-0716           298           0.5         0.144                1                   1                       1\n",
      "data_S12-0803           162           0.5         0.154                1                   1                       1\n",
      "data_S04-0628           372           0.5         0.212                0.98                0.994                   0.979\n",
      "data_S09-0628           326           0.5         0.199                0.985               0.996                   0.984\n",
      "\u001b[1mSPATIAL\u001b[0m\n",
      "Name             #Samples      Tr_split      Te_split      Train ErrP Prec      Train ErrP Acc      Train Non_ErrP Acc\n",
      "-------------  ------------  ------------  ------------  -------------------  ------------------  ----------------------\n",
      "data_S15-0909           324           0.5         0.241                0.944               0.987                   0.944\n",
      "data_S07-0729           461           0.5         0.191                0.966               0.982                   0.967\n",
      "data_S01-0717           218           0.5         0.188                0.954               0.992                   0.953\n",
      "data_S07-0718           290           0.5         0.355                0.965               0.97                    0.964\n",
      "data_S02-0717           254           0.5         0.425                0.941               0.969                   0.942\n",
      "data_S01-0719           296           0.5         0.375                0.941               0.974                   0.94\n",
      "data_S16-0910           409           0.5         0.205                0.954               0.986                   0.952\n",
      "data_S06-0801           400           0.5         0.198                0.92                0.98                    0.916\n",
      "data_S01-0629           375           0.5         0.216                0.959               0.987                   0.961\n",
      "data_S02-0716           290           0.5         0.228                0.941               0.98                    0.94\n",
      "data_S03-0628           400           0.5         0.2                  0.928               0.955                   0.931\n",
      "data_S07-0626           451           0.5         0.222                0.981               0.982                   0.981\n",
      "data_S02-0718           220           0.5         0.218                0.955               0.988                   0.955\n",
      "data_S09-0627           113           0.5         0.177                0.985               0.998                   0.984\n",
      "data_S05-0628           261           0.5         0.153                0.951               0.994                   0.95\n",
      "data_S01-0724           488           0.5         0.242                0.959               0.988                   0.958\n",
      "data_S08-0807           195           0.5         0.282                0.957               0.966                   0.957\n",
      "data_S07-0716           298           0.5         0.144                0.991               0.996                   0.991\n",
      "data_S12-0803           162           0.5         0.154                0.969               0.999                   0.967\n",
      "data_S04-0628           372           0.5         0.212                0.918               0.982                   0.917\n",
      "data_S09-0628           326           0.5         0.199                0.914               0.993                   0.905\n",
      "\u001b[1mFREQ\u001b[0m\n",
      "Name             #Samples      Tr_split      Te_split      Train ErrP Prec      Train ErrP Acc      Train Non_ErrP Acc\n",
      "-------------  ------------  ------------  ------------  -------------------  ------------------  ----------------------\n",
      "data_S15-0909           324           0.5         0.241                0.925               0.94                    0.942\n",
      "data_S07-0729           461           0.5         0.191                0.942               0.949                   0.95\n",
      "data_S01-0717           218           0.5         0.188                0.938               0.985                   0.951\n",
      "data_S07-0718           290           0.5         0.355                0.946               0.923                   0.953\n",
      "data_S02-0717           254           0.5         0.425                0.927               0.911                   0.933\n",
      "data_S01-0719           296           0.5         0.375                0.94                0.94                    0.949\n",
      "data_S16-0910           409           0.5         0.205                0.927               0.959                   0.931\n",
      "data_S06-0801           400           0.5         0.198                0.89                0.922                   0.912\n",
      "data_S01-0629           375           0.5         0.216                0.939               0.961                   0.942\n",
      "data_S02-0716           290           0.5         0.228                0.937               0.973                   0.949\n",
      "data_S03-0628           400           0.5         0.2                  0.867               0.921                   0.9\n",
      "data_S07-0626           451           0.5         0.222                0.973               0.977                   0.976\n",
      "data_S02-0718           220           0.5         0.218                0.97                0.992                   0.976\n",
      "data_S09-0627           113           0.5         0.177                0.998               1                       0.999\n",
      "data_S05-0628           261           0.5         0.153                0.884               0.961                   0.917\n",
      "data_S01-0724           488           0.5         0.242                0.935               0.951                   0.935\n",
      "data_S08-0807           195           0.5         0.282                0.862               0.902                   0.934\n",
      "data_S07-0716           298           0.5         0.144                0.996               1                       0.996\n",
      "data_S12-0803           162           0.5         0.154                1                   1                       1\n",
      "data_S04-0628           372           0.5         0.212                0.783               0.868                   0.87\n",
      "data_S09-0628           326           0.5         0.199                0.882               0.924                   0.91\n",
      "\u001b[1mTIME\u001b[0m\n",
      "Name             #Samples      Tr_split      Te_split      Train ErrP Prec      Train ErrP Acc      Train Non_ErrP Acc\n",
      "-------------  ------------  ------------  ------------  -------------------  ------------------  ----------------------\n",
      "data_S15-0909           324           0.5         0.241                0.966               0.957                   0.965\n",
      "data_S07-0729           461           0.5         0.191                0.967               0.957                   0.967\n",
      "data_S01-0717           218           0.5         0.188                0.971               0.96                    0.971\n",
      "data_S07-0718           290           0.5         0.355                0.975               0.971                   0.975\n",
      "data_S02-0717           254           0.5         0.425                0.946               0.938                   0.946\n",
      "data_S01-0719           296           0.5         0.375                0.962               0.95                    0.962\n",
      "data_S16-0910           409           0.5         0.205                0.965               0.973                   0.964\n",
      "data_S06-0801           400           0.5         0.198                0.958               0.949                   0.957\n",
      "data_S01-0629           375           0.5         0.216                0.956               0.96                    0.955\n",
      "data_S02-0716           290           0.5         0.228                0.938               0.937                   0.936\n",
      "data_S03-0628           400           0.5         0.2                  0.933               0.921                   0.93\n",
      "data_S07-0626           451           0.5         0.222                0.967               0.964                   0.967\n",
      "data_S02-0718           220           0.5         0.218                0.968               0.953                   0.968\n",
      "data_S09-0627           113           0.5         0.177                0.992               0.994                   0.992\n",
      "data_S05-0628           261           0.5         0.153                0.971               0.978                   0.97\n",
      "data_S01-0724           488           0.5         0.242                0.979               0.97                    0.979\n",
      "data_S08-0807           195           0.5         0.282                0.977               0.951                   0.977\n",
      "data_S07-0716           298           0.5         0.144                0.996               0.994                   0.996\n",
      "data_S12-0803           162           0.5         0.154                0.983               0.992                   0.983\n",
      "data_S04-0628           372           0.5         0.212                0.935               0.956                   0.932\n",
      "data_S09-0628           326           0.5         0.199                0.96                0.975                   0.958\n"
     ]
    }
   ],
   "source": [
    "print_results_train, print_results_test = {}, {}\n",
    "\n",
    "for key_model in cmat_train.keys():\n",
    "    print_results_train[key_model] = []\n",
    "    print_results_test[key_model] = []\n",
    "        \n",
    "for key_exp in result_dict.keys():\n",
    "    [cmat_train, cmat_test, auc_test, num_samples, tr_sp, te_sp, Y_test, Y_pred] = result_dict[key_exp]\n",
    "    for key_model in cmat_train.keys():\n",
    "        train_prec = cmat_train[key_model][1,1]*1.0/(cmat_train[key_model][1,1] + cmat_train[key_model][2,1])\n",
    "        test_prec = cmat_test[key_model][1,1]*1.0/(cmat_test[key_model][1,1] + cmat_test[key_model][2,1])\n",
    "        train_acc = cmat_train[key_model][1:,1:].diagonal()*1.0/cmat_train[key_model][1:,1:].sum(axis=1)\n",
    "        test_acc = cmat_test[key_model][1:,1:].diagonal()*1.0/cmat_test[key_model][1:,1:].sum(axis=1)\n",
    "        test_perDN = cmat_train[key_model][1:,0]*1.0/sum(cmat_train[key_model][1:,:], axis=1) # percentage of Don't KNOW\n",
    "        test_auc = auc_test[key_model]\n",
    "        f1score_test = 2*(test_acc[0])*test_prec/(test_acc[0] + test_prec)\n",
    "        print_results_test[key_model].append([key_exp, '%.3f'%mean(test_acc), '%.3f'%test_prec, '%.3f'%test_auc, '%.3f'%mean(f1score_test), '%.3f'%test_acc[0], '%.3f'%test_acc[1], '%.3f'%mean(test_perDN), '%.3f'%test_perDN[0], '%.3f'%test_perDN[1]])\n",
    "        print_results_train[key_model].append([key_exp, '%.3f'%num_samples, '%.3f'%tr_sp, '%.3f'%te_sp, '%.3f'%train_prec ,'%.3f'%train_acc[0], '%.3f'%train_acc[1]])\n",
    "    \n",
    "    \n",
    "    # Plot error distribution in time\n",
    "    if plt_err_dist and flag_test:\n",
    "        decision = (Y_pred == Y_test)\n",
    "        x = [ind_x for ind_x in range(len(Y_test))]\n",
    "        x = np.array(x)\n",
    "        correct_idx = np.argwhere(decision==True)\n",
    "        incorrect_idx = np.argwhere(decision==False)\n",
    "        plt.plot(x[correct_idx],Y_test[correct_idx],'b.',label='Correct')\n",
    "        plt.plot(x[incorrect_idx],Y_test[incorrect_idx],'r.',label='Incorrect')\n",
    "        plt.xlabel('timesteps')\n",
    "        yticks(np.array([-5, 0, 1, 5]), ('','ErrP', 'Non-Errp',''))\n",
    "        plt.legend()\n",
    "        vert_line = 100\n",
    "        while vert_line < len(Y_test):\n",
    "            plt.axvline(x=vert_line)\n",
    "            vert_line = vert_line + 100\n",
    "        plt.title(key)\n",
    "        plt.show()\n",
    "\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "print(color.BLUE + \"TEST\" + color.END)\n",
    "# print()\n",
    "# for key_model in print_results_test.keys():        \n",
    "#     print color.BOLD + key_model.upper() + color.END\n",
    "#     print tabulate(list(print_results_test[key_model]), headers=['Name', 'ErrP Prec', 'ErrP Acc', 'NonErrP Acc', 'Avg. %DN', '%DN (from ErrP)', '%DN (from NonErrP)'])\n",
    "#     print \"\"\n",
    "\n",
    "print(\"Name, Avg Accuracy, ErrP Precision, AUC Score, F1, ErrP Acc, NonErrP Acc, Avg DN, ErrP DN, NonErrP DN\")\n",
    "# print()\n",
    "for key_model in print_results_test.keys():  \n",
    "    print(color.BOLD + key_model.upper() + color.END)\n",
    "    for key_sub in print_results_test[key_model]:\n",
    "        for item in key_sub:\n",
    "            print(\"{}, \".format(item),end = '')\n",
    "        print(\"\")\n",
    "    \n",
    "# print()\n",
    "print(color.RED + \"TRAIN\" + color.END)\n",
    "# print()\n",
    "for key_model in print_results_train.keys():\n",
    "    print(color.BOLD + key_model.upper() + color.END)\n",
    "    print(tabulate(list(print_results_train[key_model]), headers=['Name', '#Samples  ', 'Tr_split  ', 'Te_split  ', 'Train ErrP Prec  ', 'Train ErrP Acc  ', 'Train Non_ErrP Acc  ']))\n",
    "#     print()\n",
    "    \n",
    "# print color.RED + \"TRAIN\" + color.END  \n",
    "# print \"\"\n",
    "# for key_model in print_results_test.keys():\n",
    "#     print color.BOLD + key_model.upper() + color.END\n",
    "#     print tabulate(list(print_results_train[key_model]), headers=['Name', '#Samples  ', 'Tr_split  ', 'Te_split  ', 'Train ErrP Prec  ', 'Train ErrP Acc  ', 'Train Non_ErrP Acc  '])\n",
    "#     print \"\"\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Confusion matrix format\n",
    "#      PRED-- ErrP - Non-ErrP\n",
    "# TRUE     --------------\n",
    "# ErrP     --  TP -- FN\n",
    "# NonErrP  --  FP -- TN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
