{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Written and Copyright by Mohit Agarwal\n",
    "#### Georgia Institute of Technology\n",
    "#### Email: magarwal37@gatech.edu\n",
    "\n",
    "### Code to decode Error-Potentials for the data recording on any game\n",
    "### Data should be present in the format of SXX-MMDD-XX.csv, _labels, and _metadata\n",
    "### 16 Electrode Channels: desribed in metadata file e.g. ['Pz', 'F4', 'C4', 'P4', 'O2', 'F8', 'T4', 'Cz', 'Fz', 'F3', 'C3', 'P3', 'O1', 'F7', 'T3', 'Fpz']\n",
    "\n",
    "### If want to classify data for every subject individually - put separate folder for every subject\n",
    "### If training and testing on different files, the folder structure should be similar in both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "##Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bisect\n",
    "import os\n",
    "import math\n",
    "from scipy.signal import *\n",
    "from pylab import *\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from random import shuffle\n",
    "from sklearn import svm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyriemann\n",
    "from tabulate import tabulate\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixed Parameters of the Algorithm\n",
    "data_dir = 'data'\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "channels = ['Pz', 'F4', 'C4', 'P4', 'O2', 'F8', 'Fp2', 'Cz', 'Fz', 'F3', 'C3', 'P3', 'O1', 'F7', 'Fp1', 'Fpz']\n",
    "\n",
    "stim_nonErrp = 1\n",
    "stim_Errp = 0\n",
    "th_1 = 75 # in uV : for adjacent spikes\n",
    "th_2 = 150 # in uV : for min-max\n",
    "freq = 125.0\n",
    "time_delay = 0.0 # in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters of the algorithm [to change]\n",
    "\n",
    "# Misc\n",
    "flag_test = False  # True if testing o.w. Cross-validate\n",
    "plt_err_dist = False  # Plots Error Distribution in time if set True\n",
    "selected_channels = [0,1,2,3,7,8,9,10,11,15] # If empty means all channels are selected, other option: [0,4,5]\n",
    "balanced_sampling = \"undersample\" # options are \"undersample\", \"oversample\", \"smote, \"None\"\n",
    "sync_method = \"event_date\" # options are \"default\" and \"event_date\"\n",
    "\n",
    "# Classification based\n",
    "low_freq = 1.0\n",
    "baseline_epoc = int(0.1*freq)\n",
    "butter_filt_order = 4\n",
    "cv_folds = 10\n",
    "xdawn_filters = 4\n",
    "algo = \"ner\" # options are mit,ner,custom\n",
    "\n",
    "# Plot-based\n",
    "plot_GAERP_allchan = False\n",
    "plot_GAERP_onechan = False\n",
    "plot_ERP_onechan = False\n",
    "chan_toPlot = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Per-Processing and Classification parameters as per the chosen algorithm\n",
    "\n",
    "if algo is \"mit\":\n",
    "    mean_baseline = False\n",
    "    mean_per_chan = False\n",
    "    car_pre = False\n",
    "    epoc_window = int(0.8*freq)\n",
    "    high_freq = 40.0\n",
    "    car_post = True\n",
    "    class_model = \"mit\"\n",
    "elif algo is \"ner\":\n",
    "    mean_baseline = False\n",
    "    mean_per_chan = True\n",
    "    car_pre = False\n",
    "    epoc_window = int(1.3*freq)\n",
    "    high_freq = 40.0\n",
    "    car_post = False\n",
    "    class_model = \"ner\"\n",
    "else:\n",
    "    mean_baseline = True\n",
    "    mean_per_chan = False\n",
    "    car_pre = False#True\n",
    "    epoc_window = int(0.8*freq)\n",
    "    high_freq = 10.0\n",
    "    car_post = False\n",
    "    class_model = \"ner\"\n",
    "\n",
    "total_channels = 16 if selected_channels is None else len(selected_channels) \n",
    "if selected_channels:\n",
    "    channels = [channels[i] for i in selected_channels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "# function to convert missing values to 0\n",
    "convert = lambda x: float(x.strip() or float('NaN'))\n",
    "\n",
    "def convert_codes(x):\n",
    "    if ':' in x:\n",
    "        return x.split(':',1)[1]\n",
    "    else:\n",
    "        return (x.strip() or float('NaN'))\n",
    "\n",
    "# function to convert stimulations\n",
    "def to_byte(value, length):\n",
    "    for x in range(length):\n",
    "        yield value%256\n",
    "        value//=256\n",
    "\n",
    "# function to bandpass filter\n",
    "def bandpass(sig,band,fs,butter_filt_order):\n",
    "    B,A = butter(butter_filt_order, np.array(band)/(fs/2), btype='bandpass')\n",
    "    return lfilter(B, A, sig, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(DataFolder, label_file):\n",
    "    raw_file = label_file.split('_')[0] + '.csv'\n",
    "    raw_EEG = np.loadtxt(open(os.path.join(DataFolder,raw_file), \"rb\"), delimiter=\",\", skiprows=1, usecols=(0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17))\n",
    "    stim = pd.read_csv(os.path.join(DataFolder,label_file))\n",
    "    return raw_EEG, stim\n",
    "\n",
    "def pre_process(raw_EEG):\n",
    "    # Step 1: Subtract Mean per electrode\n",
    "    sig = raw_EEG[:,1:]\n",
    "    sig = sig[:,selected_channels] if selected_channels is not None else sig\n",
    "    if mean_per_chan:\n",
    "        sig_mean = np.mean(sig, axis=0)\n",
    "        sig = sig - sig_mean\n",
    "        \n",
    "    # Step 2: Bandpass in the given frequency range\n",
    "    sigF = bandpass(sig, [low_freq,high_freq], freq, butter_filt_order)\n",
    "    \n",
    "    # Step 3: Removing Average of all channels\n",
    "    if car_pre:\n",
    "        sig_mean = np.mean(sigF, axis=1)\n",
    "        sigF = sigF - np.reshape(sig_mean, [len(sig_mean),1])*np.ones([1,total_channels])\n",
    "    \n",
    "    return sigF\n",
    "\n",
    "def post_process(X):\n",
    "    if car_post:\n",
    "        temp =  np.repeat(np.mean(X,axis=2),X.shape[2],axis=1)\n",
    "        temp = np.reshape(temp, X.shape)\n",
    "        X = X - temp\n",
    "    X = X.transpose((0,2,1))\n",
    "    return X\n",
    "    \n",
    "\n",
    "# Arrange EEG data for training/testing\n",
    "def arrange_data(EEG, sigF, stim):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for stim_id, stim_code in enumerate(stim['label']):\n",
    "        if (not math.isnan(stim_code)) and stim_code!=33552 and stim_code!=33553 and stim_code!=33554:\n",
    "            if stim_code==stim_nonErrp or stim_code==stim_Errp:\n",
    "                if sync_method == \"default\":\n",
    "                    time_instant = stim['time1'][stim_id] + time_delay\n",
    "                else:\n",
    "                    time_instant = stim['time2'][stim_id] + time_delay\n",
    "                time_instant = time_instant\n",
    "                idx = bisect.bisect(EEG[:,0],time_instant)\n",
    "                X_temp = sigF[idx-1:idx+epoc_window,:]\n",
    "                if mean_baseline:\n",
    "                    X_mean = np.mean(sigF[idx-baseline_epoc:idx,:],0)\n",
    "                    X_temp = X_temp - X_mean\n",
    "                check = False\n",
    "                # Removing nan values and corrupted data\n",
    "                if np.isnan(X_temp).any():\n",
    "                    check = True\n",
    "                for i in range(total_channels):  \n",
    "                    X_diff = [abs(t - s) for s, t in zip(X_temp[:,i], X_temp[1:,i])]\n",
    "                    if max(X_diff)>th_1 or max(X_temp[:,i])-min(X_temp[:,i])>th_2:\n",
    "                        check = True\n",
    "                if not check:\n",
    "                    X.append(X_temp)\n",
    "                    Y.append(stim_code)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S04-0628-06_labels.csv', 'S04-0628-02_labels.csv', 'S04-0628-08_labels.csv', 'S04-0628-03_labels.csv', 'S04-0628-04_labels.csv', 'S04-0628-05_labels.csv', 'S04-0628-07_labels.csv', 'S04-0628-10_labels.csv', 'S04-0628-09_labels.csv', 'S04-0628-01_labels.csv']\n",
      "['S01-0719-06_labels.csv', 'S01-0719-05_labels.csv', 'S01-0719-02_labels.csv', 'S01-0719-03_labels.csv', 'S01-0719-04_labels.csv', 'S01-0719-01_labels.csv']\n",
      "['S15-0909-05_labels.csv', 'S15-0909-07_labels.csv', 'S15-0909-10_labels.csv', 'S15-0909-04_labels.csv', 'S15-0909-06_labels.csv', 'S15-0909-09_labels.csv', 'S15-0909-02_labels.csv', 'S15-0909-01_labels.csv', 'S15-0909-03_labels.csv', 'S15-0909-08_labels.csv']\n",
      "['S06-0801-08_labels.csv', 'S06-0801-10_labels.csv', 'S06-0801-04_labels.csv', 'S06-0801-03_labels.csv', 'S06-0801-06_labels.csv', 'S06-0801-02_labels.csv', 'S06-0801-09_labels.csv', 'S06-0801-07_labels.csv', 'S06-0801-01_labels.csv', 'S06-0801-05_labels.csv']\n",
      "['S03-0628-03_labels.csv', 'S03-0628-04_labels.csv', 'S03-0628-02_labels.csv', 'S03-0628-05_labels.csv', 'S03-0628-01_labels.csv']\n",
      "['S12-0803-01_labels.csv', 'S12-0803-05_labels.csv', 'S12-0803-04_labels.csv', 'S12-0803-03_labels.csv', 'S12-0803-02_labels.csv']\n",
      "['S07-0718-06_labels.csv', 'S07-0718-01_labels.csv', 'S07-0718-05_labels.csv', 'S07-0718-04_labels.csv', 'S07-0718-02_labels.csv', 'S07-0718-03_labels.csv']\n",
      "['S16-0910-01_labels.csv', 'S16-0910-03_labels.csv', 'S16-0910-10_labels.csv', 'S16-0910-07_labels.csv', 'S16-0910-05_labels.csv', 'S16-0910-02_labels.csv', 'S16-0910-04_labels.csv', 'S16-0910-08_labels.csv', 'S16-0910-09_labels.csv', 'S16-0910-06_labels.csv']\n",
      "['S08-0807-04_labels.csv', 'S08-0807-02_labels.csv', 'S08-0807-06_labels.csv', 'S08-0807-10_labels.csv', 'S08-0807-01_labels.csv', 'S08-0807-05_labels.csv', 'S08-0807-07_labels.csv', 'S08-0807-09_labels.csv', 'S08-0807-08_labels.csv', 'S08-0807-03_labels.csv']\n",
      "['S07-0729-02_labels.csv', 'S07-0729-01_labels.csv', 'S07-0729-06_labels.csv', 'S07-0729-03_labels.csv', 'S07-0729-05_labels.csv', 'S07-0729-04_labels.csv']\n",
      "['S01-0724-06_labels.csv', 'S01-0724-02_labels.csv', 'S01-0724-04_labels.csv', 'S01-0724-03_labels.csv', 'S01-0724-05_labels.csv', 'S01-0724-01_labels.csv']\n",
      "['S02-0717-03_labels.csv', 'S02-0717-05_labels.csv', 'S02-0717-04_labels.csv', 'S02-0717-02_labels.csv', 'S02-0717-06_labels.csv', 'S02-0717-01_labels.csv']\n",
      "['S01-0629-11_labels.csv', 'S01-0629-10_labels.csv', 'S01-0629-05_labels.csv', 'S01-0629-09_labels.csv', 'S01-0629-06_labels.csv', 'S01-0629-08_labels.csv', 'S01-0629-02_labels.csv', 'S01-0629-03_labels.csv', 'S01-0629-04_labels.csv', 'S01-0629-01_labels.csv', 'S01-0629-07_labels.csv']\n",
      "['S09-0627-01_labels.csv', 'S09-0627-02_labels.csv', 'S09-0627-03_labels.csv']\n",
      "['S09-0628-03_labels.csv', 'S09-0628-02_labels.csv', 'S09-0628-04_labels.csv', 'S09-0628-05_labels.csv', 'S09-0628-01_labels.csv']\n",
      "['S01-0717-01_labels.csv', 'S01-0717-03_labels.csv', 'S01-0717-05_labels.csv', 'S01-0717-06_labels.csv', 'S01-0717-04_labels.csv', 'S01-0717-02_labels.csv', 'S01-0717-08_labels.csv', 'S01-0717-07_labels.csv', 'S01-0717-10_labels.csv', 'S01-0717-09_labels.csv']\n",
      "['S02-0718-04_labels.csv', 'S02-0718-02_labels.csv', 'S02-0718-05_labels.csv', 'S02-0718-06_labels.csv', 'S02-0718-03_labels.csv', 'S02-0718-01_labels.csv']\n",
      "['S02-0716-03_labels.csv', 'S02-0716-08_labels.csv', 'S02-0716-04_labels.csv', 'S02-0716-05_labels.csv', 'S02-0716-09_labels.csv', 'S02-0716-10_labels.csv', 'S02-0716-07_labels.csv', 'S02-0716-06_labels.csv', 'S02-0716-01_labels.csv', 'S02-0716-02_labels.csv']\n",
      "['S07-0626-02_labels.csv', 'S07-0626-04_labels.csv', 'S07-0626-07_labels.csv', 'S07-0626-08_labels.csv', 'S07-0626-01_labels.csv', 'S07-0626-06_labels.csv', 'S07-0626-03_labels.csv', 'S07-0626-05_labels.csv']\n",
      "['S07-0716-04_labels.csv', 'S07-0716-10_labels.csv', 'S07-0716-02_labels.csv', 'S07-0716-03_labels.csv', 'S07-0716-01_labels.csv', 'S07-0716-05_labels.csv', 'S07-0716-06_labels.csv', 'S07-0716-08_labels.csv', 'S07-0716-09_labels.csv', 'S07-0716-07_labels.csv']\n",
      "['S05-0628-01_labels.csv', 'S05-0628-02_labels.csv', 'S05-0628-05_labels.csv', 'S05-0628-04_labels.csv', 'S05-0628-03_labels.csv']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing Data from Train/Test Directory\n",
    "\n",
    "def dir_list(folder_path):\n",
    "    dir_list = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    dir_list.append('')\n",
    "    return dir_list\n",
    "\n",
    "data_dict = {}\n",
    "train_folder = os.path.join(data_dir,train_dir)\n",
    "test_folder = os.path.join(data_dir,test_dir)\n",
    "list_of_dir = dir_list(train_folder)\n",
    "if flag_test:\n",
    "    assert list_of_dir==dir_list(test_folder), \"Error: Test directory does not have same structure as train\"\n",
    "total_dirs = len(list_of_dir)\n",
    "for dir_idx in range(total_dirs):\n",
    "    paths = [[\"train\", os.path.join(train_folder, list_of_dir[dir_idx])], [\"test\", os.path.join(test_folder, list_of_dir[dir_idx])]]\n",
    "    if not flag_test:\n",
    "        paths.pop(1)\n",
    "    for items in paths:\n",
    "        train_test = items[0]\n",
    "        folder = items[1]\n",
    "        list_of_files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and '_labels' in f]\n",
    "        print(list_of_files)\n",
    "        for curr_file in list_of_files:\n",
    "            \n",
    "            # Step 1: Load Raw EEG data (with filtered stimulations)\n",
    "            EEG, stim = loadData(folder, curr_file)\n",
    "            \n",
    "            # Step 2: Pre-process the raw EEG data before cutting in time-windows\n",
    "            sig = pre_process(EEG)\n",
    "            \n",
    "            # Step 3: Arrange Data\n",
    "            X_curr, Y_curr = arrange_data(EEG, sig, stim)\n",
    "            \n",
    "            # Making sure either the stimulations are present or raw data during stimulation is not discarded\n",
    "            if Y_curr.shape[0] > 0:\n",
    "                # Step 4: Post-process the Signals\n",
    "                X_curr = post_process(X_curr)\n",
    "\n",
    "                # Step 5: Arrange data in dictionary\n",
    "                key = 'data_' + list_of_dir[dir_idx] + '$' + train_test\n",
    "                if key in data_dict:\n",
    "                    [X, Y] = data_dict[key]\n",
    "                    X = np.concatenate((X,X_curr),axis=0)\n",
    "                    Y = np.concatenate((Y,Y_curr),axis=0)\n",
    "                    data_dict[key] = [X, Y]\n",
    "    #                 print X.shape, Y.shape\n",
    "                else:\n",
    "                    data_dict[key] = [X_curr, Y_curr]\n",
    "                \n",
    "            \n",
    "exps = set()\n",
    "for key in data_dict.keys():\n",
    "    exps.add(key.split(\"$\")[0])\n",
    "\n",
    "# elements in the 'exps' set holds the folder names\n",
    "# [X_train, Y_train] = data_dict[exp+\"$train\"] loads the X_train and Y_train for training samples\n",
    "# All samples in all files inside one folder are concatenated when storing in data_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Visualizing Grand Average ErrP [edit it later - as per new format]\n",
    "    \n",
    "# Plot Difference of Grand Average ErrPs and non-ErrPs of all channels\n",
    "if plot_GAERP_allchan:\n",
    "    plt.rcParams['figure.figsize'] = [20, 20]\n",
    "    fig, axes = plt.subplots((total_channels+1)//2, 2, sharex=True)\n",
    "    channel_indices = range(0, total_channels) #if selected_channels is None else selected_channels\n",
    "    for chan_idx in channel_indices:\n",
    "        plot_idx_x = chan_idx//2\n",
    "        plot_idx_y = chan_idx%2\n",
    "        mean_sig = []\n",
    "        for exp in exps:\n",
    "            [X, Y] = data_dict[exp+\"$train\"]\n",
    "            idx_nonErrp = np.argwhere(Y==stim_nonErrp)\n",
    "            idx_Errp = np.argwhere(Y==stim_Errp)\n",
    "            time = [x*1.0/freq for x in range(0,X.shape[2])]\n",
    "            time = np.array(time)\n",
    "            sig_nonErrp = np.mean(X[idx_nonErrp,chan_idx,:],axis=0)\n",
    "            sig_Errp = np.mean(X[idx_Errp,chan_idx,:],axis=0)\n",
    "            axes[plot_idx_x, plot_idx_y].plot(time, sig_Errp[0] - sig_nonErrp[0], linewidth=0.8, label=exp)\n",
    "            mean_sig.append(sig_Errp[0] - sig_nonErrp[0])\n",
    "        avg_signal = np.mean(np.array(mean_sig), axis=0)\n",
    "        axes[plot_idx_x, plot_idx_y].plot(time, avg_signal, color='k', linewidth=2.0, label = \"Average\")\n",
    "        axes[plot_idx_x, plot_idx_y].legend()\n",
    "        axes[plot_idx_x, plot_idx_y].set_xlabel('time (in s)')\n",
    "        axes[plot_idx_x, plot_idx_y].set_ylabel('Amplitude[uV]')\n",
    "        axes[plot_idx_x, plot_idx_y].set_title(r\"$\\bf{\" +  \"Channel: \" + channels[chan_idx] + \"}$\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Plot Difference of Grand Average ErrPs and non-ErrPs of a single channel [Error-correct]\n",
    "if plot_GAERP_onechan:\n",
    "    plt.rcParams['figure.figsize'] = [8, 4]\n",
    "    fig, axes = plt.subplots()\n",
    "    mean_sig = []\n",
    "    for exp in exps:\n",
    "        [X, Y] = data_dict[exp+\"$train\"]\n",
    "        idx_nonErrp = np.argwhere(Y==stim_nonErrp)\n",
    "        idx_Errp = np.argwhere(Y==stim_Errp)\n",
    "        time = [x*1.0/freq for x in range(0,X.shape[2])]\n",
    "        time = np.array(time)\n",
    "        sig_nonErrp = np.mean(X[idx_nonErrp,chan_toPlot,:],axis=0)\n",
    "        sig_Errp = np.mean(X[idx_Errp,chan_toPlot,:],axis=0)\n",
    "        axes.plot(time, sig_Errp[0] - sig_nonErrp[0], linewidth=0.8, label=exp)\n",
    "        mean_sig.append(sig_Errp[0] - sig_nonErrp[0])\n",
    "    avg_signal = np.mean(np.array(mean_sig), axis=0)\n",
    "    axes.plot(time, avg_signal, color='k', linewidth=2.0, label = \"Average\")\n",
    "    axes.legend()\n",
    "    axes.set_xlabel('time (in s)')\n",
    "    axes.set_ylabel('Amplitude[uV]')\n",
    "    axes.set_title(\"Channel: \" + channels[chan_toPlot], fontname=\"Times New Roman Bold\")\n",
    "    plt.show()\n",
    "\n",
    "# Experimental/Research exploration for ErrP plotting \n",
    "if plot_ERP_onechan:\n",
    "    plt.rcParams['figure.figsize'] = [8,4]\n",
    "    fig, axes = plt.subplots()\n",
    "    exp = next(iter(exps))\n",
    "    [X, Y] = data_dict[exp+\"$train\"]\n",
    "    idx_nonErrp = np.argwhere(Y==stim_nonErrp)\n",
    "    idx_Errp = np.argwhere(Y==stim_Errp)\n",
    "    time = [x*1.0/freq for x in range(0,X.shape[2])]\n",
    "    time = np.array(time)\n",
    "    \n",
    "    total_nonErrP = 10 # len(idx_nonErrp)\n",
    "    total_ErrP = 10 # len(idx_Errp)\n",
    "    for ind in range(0, total_nonErrP):\n",
    "        plt.plot(time, X[idx_nonErrp[ind][0],chan_toPlot,:],'b')\n",
    "    for ind in range(0, total_ErrP):\n",
    "        plt.plot(time, X[idx_Errp[ind][0],chan_toPlot,:],'r')\n",
    "    plt.title(\"Experiment: \" + exp + \"  Channel: \" + channels[chan_toPlot])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Classification Pipelines after pre-processing\n",
    "\n",
    "def pipeline_custom(X_train, X_test, Y_train):\n",
    "    \n",
    "    filt_1 = pyriemann.estimation.XdawnCovariances(nfilter=xdawn_filters, estimator='lwf', xdawn_estimator='lwf')\n",
    "    #filt_3 = pyriemann.tangentspace.TangentSpace(metric='logeuclid', tsupdate=False)\n",
    "    #filt_5 = ElasticNet(l1_ratio=0.05, alpha=0.02, normalize=False) #KNeighborsClassifier(n_neighbors=5) #svm.SVC(kernel='linear', gamma='scale')\n",
    "    filt_5 = pyriemann.classification.MDM(metric='riemann')\n",
    "                                                           \n",
    "    clf = make_pipeline(filt_1, filt_5)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    train_scores = clf.predict(X_train)\n",
    "                                                           \n",
    "    # Testing Data\n",
    "    test_scores = clf.predict(X_test)\n",
    "    return train_scores, test_scores\n",
    "\n",
    "def pipeline_ner(X_train, X_test, Y_train):\n",
    "    \n",
    "    filt_1 = pyriemann.estimation.XdawnCovariances(nfilter=xdawn_filters, applyfilters=False, estimator='lwf', xdawn_estimator='lwf')\n",
    "    filt_2 = pyriemann.channelselection.ElectrodeSelection(nelec=8, metric='riemann')\n",
    "    filt_3 = pyriemann.tangentspace.TangentSpace(metric='riemann', tsupdate=False) # 36-sized vector for nelec=8\n",
    "    filt_4 = Normalizer(norm='l1')                                                       \n",
    "    filt_5 = ElasticNet(l1_ratio=0.05, alpha=0.02, normalize=True)\n",
    "#     filt_5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(16, 4))\n",
    "#     filt_5 = SGDClassifier(loss='squared_hinge', penalty='elasticnet', l1_ratio=0.5, alpha=0.002, max_iter=1000, tol=0.001)\n",
    "#     filt_5 = \n",
    "                                                           \n",
    "#     clf = make_pipeline(filt_1, filt_2, filt_3, filt_4)\n",
    "#     clf.fit(X_train, Y_train)\n",
    "#     train_features = clf.transform(X_train)\n",
    "#     test_features = clf.transform(X_test)\n",
    "    \n",
    "#     clf_isotonic = CalibratedClassifierCV(filt_5, cv=2, method='isotonic')\n",
    "#     clf_isotonic.fit(train_features, Y_train)\n",
    "#     train_scores = clf_isotonic.predict_proba(train_features)[:, 1]\n",
    "#     test_scores = clf_isotonic.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "#     X_train = X_train[:,:8,:]\n",
    "#     print \"xtrain: \", X_train.shape\n",
    "#     filt_x = pyriemann.spatialfilters.Xdawn(nfilter=4)\n",
    "#     temp = filt_x.fit_transform(X_train, Y_train)\n",
    "#     print \"xdawn: \", temp.shape\n",
    "# #     print filt_x.evokeds_.shape\n",
    "    \n",
    "#     filt_y = pyriemann.estimation.XdawnCovariances(nfilter=4, applyfilters=False)\n",
    "#     temp2 = filt_y.fit_transform(X_train, Y_train)\n",
    "#     print \"xdawncov: \", temp2.shape\n",
    "    \n",
    "    clf = make_pipeline(filt_1, filt_2, filt_3, filt_4, filt_5)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    train_scores = clf.predict(X_train)\n",
    "                                                           \n",
    "    # Testing Data\n",
    "    test_scores = clf.predict(X_test)\n",
    "    return train_scores, test_scores\n",
    "\n",
    "def pipeline_mit(X_train, X_test, Y_train):\n",
    "    \n",
    "    ind_0 = np.argwhere(Y_train==0).flatten()\n",
    "    ind_1 = np.argwhere(Y_train==1).flatten()\n",
    "    X_train_0 = X_train[ind_0,:,:]\n",
    "    X_train_1 = X_train[ind_1,:,:]\n",
    "    mean_class_0 = np.mean(X_train_0, axis=0)\n",
    "    mean_class_1 = np.mean(X_train_1, axis=0)\n",
    "    \n",
    "    filt_1 = pyriemann.estimation.XdawnCovariances(nfilter=xdawn_filters, estimator='lwf', xdawn_estimator='lwf')\n",
    "    filt_2 = pyriemann.tangentspace.TangentSpace(metric='logeuclid', tsupdate=False)\n",
    "    filt_3 = ElasticNet(l1_ratio=0.1, alpha=0.0002, normalize=False)\n",
    "    X_fe_1 = X_train\n",
    "    X_fe_1 = filt_1.fit_transform(X_fe_1, Y_train)\n",
    "    X_fe_1 = filt_2.fit_transform(X_fe_1, Y_train)\n",
    "    X_fe_2 = np.zeros([X_train.shape[0],16])\n",
    "    channel_indices = range(0, total_channels) if selected_channels is None else selected_channels\n",
    "    for sample_idx in range(0,X_train.shape[0]):\n",
    "        for chan_idx in channel_indices:\n",
    "            mean_corr_0 = np.corrcoef(mean_class_0[chan_idx,:],X_train[sample_idx,chan_idx,:])[0,1]\n",
    "            mean_corr_1 = np.corrcoef(mean_class_1[chan_idx,:],X_train[sample_idx,chan_idx,:])[0,1]\n",
    "            X_fe_2[sample_idx, chan_idx] = mean_corr_0 - mean_corr_1\n",
    "    X_fe = np.concatenate((X_fe_1,X_fe_2),axis=1)\n",
    "    filt_3.fit(X_fe, Y_train)\n",
    "    train_scores = filt_3.predict(X_fe)\n",
    "    \n",
    "    # Testing Data\n",
    "    Xtest_fe1 = X_test\n",
    "    Xtest_fe1 = filt_1.transform(Xtest_fe1)\n",
    "    Xtest_fe1 = filt_2.transform(Xtest_fe1)\n",
    "    Xtest_fe2 = np.zeros([X_test.shape[0],16])\n",
    "    for sample_idx in range(0,X_test.shape[0]):\n",
    "        for chan_idx in range(0,16):\n",
    "            mean_corr_0 = np.corrcoef(mean_class_0[chan_idx,:],X_test[sample_idx,chan_idx,:])[0,1]\n",
    "            mean_corr_1 = np.corrcoef(mean_class_1[chan_idx,:],X_test[sample_idx,chan_idx,:])[0,1]\n",
    "            Xtest_fe2[sample_idx, chan_idx] = mean_corr_0 - mean_corr_1\n",
    "    Xtest_fe = np.concatenate((Xtest_fe1,Xtest_fe2),axis=1)\n",
    "    test_scores = filt_3.predict(Xtest_fe)\n",
    "    return train_scores, test_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(X_train, Y_train):\n",
    "    # For balanced sampling\n",
    "    if balanced_sampling == \"undersample\":\n",
    "        rusU = RandomUnderSampler(return_indices=True)\n",
    "        X_new = np.reshape(X_train,[X_train.shape[0],X_train.shape[1]*X_train.shape[2]])\n",
    "        X_resU, Y_resU, id_rusU = rusU.fit_resample(X_new, Y_train)\n",
    "        X_new2 = np.reshape(X_resU,[X_resU.shape[0],X_train.shape[1],X_train.shape[2]])\n",
    "        X_sim = X_new2\n",
    "        Y_sim = Y_resU\n",
    "    elif balanced_sampling == \"oversample\":\n",
    "        rusO = RandomOverSampler(return_indices=True)\n",
    "        X_new = np.reshape(X_train,[X_train.shape[0],X_train.shape[1]*X_train.shape[2]])\n",
    "        X_resO, Y_resO, id_rusO = rusO.fit_resample(X_new, Y_train)\n",
    "        X_new2 = np.reshape(X_resO,[X_resO.shape[0],X_train.shape[1],X_train.shape[2]])\n",
    "        X_sim = X_new2\n",
    "        Y_sim = Y_resO\n",
    "    elif balanced_sampling == \"smote\":\n",
    "        rusS = SMOTE()\n",
    "        X_new = np.reshape(X_train,[X_train.shape[0],X_train.shape[1]*X_train.shape[2]])\n",
    "        X_resS, Y_resS = rusS.fit_resample(X_new, Y_train)\n",
    "        X_new2 = np.reshape(X_resS,[X_resS.shape[0],X_train.shape[1],X_train.shape[2]])\n",
    "        X_sim = X_new2\n",
    "        Y_sim = Y_resS\n",
    "    else:\n",
    "        X_sim = X_train\n",
    "        Y_sim = Y_train\n",
    "    return X_sim, Y_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acutal Prediction\n",
    "\n",
    "def predict(X_train, X_test, Y_train, Y_test):\n",
    "    train_preds = []\n",
    "    test_preds = []\n",
    "    if class_model==\"mit\":\n",
    "        train_scores, test_scores = pipeline_mit(X_train, X_test, Y_train)\n",
    "        thresholds = [x/100.0 for x in range(20,81,1)]\n",
    "        acc_tr = []\n",
    "        for th in thresholds:\n",
    "            acc_tr.append([np.mean((train_scores > th)==Y_train),th])\n",
    "        decision_th = max(acc_tr)[1]\n",
    "        auc_score = roc_auc_score(Y_test, test_scores)\n",
    "        train_preds =  train_scores > decision_th\n",
    "        test_preds = test_scores > decision_th\n",
    "    elif class_model==\"ner\":\n",
    "        train_scores, test_scores = pipeline_ner(X_train, X_test, Y_train)\n",
    "        thresholds = [x/100.0 for x in range(20,81,1)]\n",
    "        acc_tr = []\n",
    "        for th in thresholds:\n",
    "            acc_tr.append([np.mean((train_scores > th)==Y_train),th])\n",
    "        decision_th = max(acc_tr)[1]\n",
    "        auc_score = []\n",
    "        try:\n",
    "            auc_score.append(roc_auc_score(Y_test, test_scores))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        train_preds =  train_scores > decision_th\n",
    "        test_preds = test_scores > decision_th\n",
    "    else:\n",
    "        train_scores, test_scores = pipeline_custom(X_train, X_test, Y_train)\n",
    "        thresholds = [x/100.0 for x in range(20,81,1)]\n",
    "        acc_tr = []\n",
    "        for th in thresholds:\n",
    "            acc_tr.append([np.mean((train_scores > th)==Y_train),th])\n",
    "        decision_th = max(acc_tr)[1]\n",
    "        auc_score = roc_auc_score(Y_test, test_scores)\n",
    "        train_preds =  train_scores > decision_th\n",
    "        test_preds = test_scores > decision_th\n",
    "    return confusion_matrix(Y_train, train_preds), confusion_matrix(Y_test, test_preds), test_preds, mean(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation: Returns train and test confusion matrix for X and Y\n",
    "\n",
    "def predict_CV(X, Y):    \n",
    "    kfold = KFold(n_splits=cv_folds, shuffle=True)\n",
    "    cmat_train = np.zeros([2,2])\n",
    "    cmat_test = np.zeros([2,2])\n",
    "    auc_test = 0\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "      \n",
    "        X_train, Y_train = balance_dataset(X_train, Y_train)\n",
    "            \n",
    "        # print \"Training ErrPs: \", np.argwhere(Y_train==0).shape[0]*100.0/(X_train.shape[0]), \"%\" \n",
    "        # print \"Validation ErrPs: \", np.argwhere(Y_test==0).shape[0]*100.0/(X_test.shape[0]), \"%\"\n",
    "        cm_train, cm_test, _, auc_curr = predict(X_train, X_test, Y_train, Y_test)\n",
    "        cmat_train = cmat_train + cm_train\n",
    "        cmat_test = cmat_test + cm_test\n",
    "        auc_test = auc_test + auc_curr\n",
    "\n",
    "    return cmat_train, cmat_test, auc_test*1.0/cv_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 0/21 :data_S08-0807\n",
      "Processing.. 1/21 :data_S07-0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 2/21 :data_S04-0628\n",
      "Processing.. 3/21 :data_S06-0801\n",
      "Processing.. 4/21 :data_S05-0628\n",
      "Processing.. 5/21 :data_S16-0910\n",
      "Processing.. 6/21 :data_S02-0717\n",
      "Processing.. 7/21 :data_S07-0626\n",
      "Processing.. 8/21 :data_S01-0719\n",
      "Processing.. 9/21 :data_S01-0629\n",
      "Processing.. 10/21 :data_S01-0724\n",
      "Processing.. 11/21 :data_S07-0718\n",
      "Processing.. 12/21 :data_S12-0803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 13/21 :data_S07-0729\n",
      "Processing.. 14/21 :data_S02-0716\n",
      "Processing.. 15/21 :data_S09-0628\n",
      "Processing.. 16/21 :data_S09-0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 17/21 :data_S15-0909\n",
      "Processing.. 18/21 :data_S01-0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mohit/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.. 19/21 :data_S02-0718\n",
      "Processing.. 20/21 :data_S03-0628\n"
     ]
    }
   ],
   "source": [
    "# Computing Accuracies\n",
    "\n",
    "result_dict = {}\n",
    "cmat_train, cmat_test, auc_test = np.zeros([2,2]), np.zeros([2,2]), []\n",
    "\n",
    "for exp_idx, exp in enumerate(exps):\n",
    "    print(\"Processing.. \"+str(exp_idx)+\"/\"+str(len(exps))+\" :\"+exp)\n",
    "    Y_pred = None\n",
    "    Y_test = None\n",
    "    if flag_test:\n",
    "        [X_train, Y_train] = data_dict[exp+\"$train\"]\n",
    "        index_shuffle = range(X_train.shape[0])\n",
    "        shuffle(index_shuffle)\n",
    "        X_train = X_train[index_shuffle,:,:]\n",
    "        Y_train = Y_train[index_shuffle]\n",
    "        [X_test, Y_test] = data_dict[exp+\"$test\"]\n",
    "        X_train, Y_train = balance_dataset(X_train, Y_train)\n",
    "        cmat_train, cmat_test, Y_pred, auc_test = predict(X_train, X_test, Y_train, Y_test)\n",
    "    else:\n",
    "        cmat_train, cmat_test, auc_test = np.zeros([2,2]), np.zeros([2,2]), []\n",
    "        num_sims = 10\n",
    "        [X_train, Y_train] = data_dict[exp+\"$train\"]\n",
    "        for ind in range(num_sims):\n",
    "            a, b, c = predict_CV(X_train,Y_train)\n",
    "            cmat_train = cmat_train + a\n",
    "            cmat_test = cmat_test + b\n",
    "            auc_test.append(c)\n",
    "        cmat_train = cmat_train/num_sims\n",
    "        cmat_test = cmat_test/num_sims\n",
    "        auc_test = mean(auc_test)\n",
    "    \n",
    "    assert not exp in result_dict.keys()\n",
    "    train_errp_split = (sum(cmat_train,1)*1.0/sum(cmat_train))[0]\n",
    "    test_errp_split = (sum(cmat_test,1)*1.0/sum(cmat_test))[0]\n",
    "    result_dict[exp] = [cmat_train, cmat_test, X_train.shape[0], train_errp_split, test_errp_split, Y_test, Y_pred, auc_test]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name             Avg. Acc    Precision         AUC    F1 Score    ErrP Acc    Non_ErrP Acc\n",
      "-------------  ----------  -----------  ----------  ----------  ----------  --------------\n",
      "data_S08-0807    0.580848     0.280284    0.613495    0.380985    0.594624        0.567073\n",
      "data_S07-0716    0.855769     0.33558   nan           0.492095    0.922222        0.789316\n",
      "data_S04-0628    0.684923     0.349456    0.754601    0.459825    0.672093        0.697753\n",
      "data_S06-0801    0.728609     0.37814     0.82022     0.503389    0.752703        0.704516\n",
      "data_S05-0628    0.719779     0.310897    0.823187    0.436445    0.732075        0.707483\n",
      "data_S16-0910    0.760841     0.452276    0.844125    0.56462     0.75122         0.770462\n",
      "data_S02-0717    0.659561     0.55425     0.709824    0.605133    0.666304        0.652817\n",
      "data_S07-0626    0.858118     0.631285    0.941551    0.724691    0.850538        0.865698\n",
      "data_S01-0719    0.808542     0.712207    0.876761    0.74756     0.786607        0.830476\n",
      "data_S01-0629    0.783486     0.459405    0.871366    0.580838    0.789535        0.777437\n",
      "data_S01-0724    0.848773     0.636548    0.932987    0.722541    0.835398        0.862148\n",
      "data_S07-0718    0.730736     0.539394    0.826033    0.627708    0.750602        0.71087\n",
      "data_S12-0803    0.796003     0.484894  nan           0.60452     0.8025          0.789506\n",
      "data_S07-0729    0.795923     0.553804    0.880042    0.641524    0.762264        0.829581\n",
      "data_S02-0716    0.762469     0.534483    0.847021    0.62        0.738095        0.786842\n",
      "data_S09-0628    0.697694     0.352941    0.774111    0.47463     0.72439         0.670997\n",
      "data_S09-0627    0.676084     0.333689  nan           0.454282    0.711364        0.640805\n",
      "data_S15-0909    0.689066     0.360731    0.755678    0.473054    0.686957        0.691176\n",
      "data_S01-0717    0.746597     0.379078  nan           0.510993    0.783721        0.709474\n",
      "data_S02-0718    0.731272     0.424071    0.810747    0.544606    0.76087         0.701674\n",
      "data_S03-0628    0.628927     0.311199    0.683702    0.417832    0.635632        0.622222\n",
      "Name             #Samples    Tr_split    Te_split    Train Avg. Acc    Train ErrP Acc    Train Non_ErrP Acc\n",
      "-------------  ----------  ----------  ----------  ----------------  ----------------  --------------------\n",
      "data_S08-0807         421         0.5    0.220903          0.788112          0.787814              0.788411\n",
      "data_S07-0716         261         0.5    0.103448          0.995885          0.997119              0.99465\n",
      "data_S04-0628         442         0.5    0.19457           0.831654          0.836434              0.826873\n",
      "data_S06-0801         384         0.5    0.192708          0.86539           0.87973               0.851051\n",
      "data_S05-0628         347         0.5    0.152738          0.89434           0.892872              0.895807\n",
      "data_S16-0910         407         0.5    0.201474          0.871748          0.849729              0.893767\n",
      "data_S02-0717         234         0.5    0.393162          0.809964          0.806039              0.813889\n",
      "data_S07-0626         437         0.5    0.212815          0.914456          0.902628              0.926284\n",
      "data_S01-0719         322         0.5    0.347826          0.876835          0.84871               0.90496\n",
      "data_S01-0629         445         0.5    0.193258          0.887274          0.890698              0.88385\n",
      "data_S01-0724         504         0.5    0.224206          0.913815          0.898918              0.928712\n",
      "data_S07-0718         267         0.5    0.310861          0.812985          0.821017              0.804953\n",
      "data_S12-0803         202         0.5    0.19802           0.958889          0.965556              0.952222\n",
      "data_S07-0729         488         0.5    0.217213          0.859172          0.816247              0.902096\n",
      "data_S02-0716         253         0.5    0.249012          0.892857          0.888183              0.897531\n",
      "data_S09-0628         413         0.5    0.198547          0.81687           0.851355              0.782385\n",
      "data_S09-0627         218         0.5    0.201835          0.874495          0.898232              0.850758\n",
      "data_S15-0909         341         0.5    0.202346          0.84066           0.840419              0.840902\n",
      "data_S01-0717         233         0.5    0.184549          0.932041          0.9323                0.931783\n",
      "data_S02-0718         308         0.5    0.224026          0.865378          0.883736              0.847021\n",
      "data_S03-0628         411         0.5    0.211679          0.814496          0.813921              0.81507\n"
     ]
    }
   ],
   "source": [
    "print_results_1 = []\n",
    "print_results_2 = []\n",
    "for key in result_dict.keys():\n",
    "    [cmat_train, cmat_test, num_samples, tr_sp, te_sp, Y_test, Y_pred, auc_test] = result_dict[key]\n",
    "    train_acc = cmat_train.diagonal()*1.0/cmat_train.sum(axis=1)\n",
    "    test_acc = cmat_test.diagonal()*1.0/cmat_test.sum(axis=1)\n",
    "    test_prec = cmat_test[0,0]*1.0/(cmat_test[0,0] + cmat_test[1,0])\n",
    "    f1score = 2*test_acc[0]*test_prec/(test_acc[0] + test_prec)\n",
    "    print_results_1.append([key, mean(test_acc), test_prec, auc_test, f1score, test_acc[0],test_acc[1]])\n",
    "    print_results_2.append([key,num_samples,tr_sp, te_sp, mean(train_acc),train_acc[0], train_acc[1]])\n",
    "    \n",
    "    # Plot error distribution in time\n",
    "    if plt_err_dist and flag_test:\n",
    "        decision = (Y_pred == Y_test)\n",
    "        x = [ind_x for ind_x in range(len(Y_test))]\n",
    "        x = np.array(x)\n",
    "        correct_idx = np.argwhere(decision==True)\n",
    "        incorrect_idx = np.argwhere(decision==False)\n",
    "        plt.plot(x[correct_idx],Y_test[correct_idx],'b.',label='Correct')\n",
    "        plt.plot(x[incorrect_idx],Y_test[incorrect_idx],'r.',label='Incorrect')\n",
    "        plt.xlabel('timesteps')\n",
    "        yticks(np.array([-5, 0, 1, 5]), ('','ErrP', 'Non-Errp',''))\n",
    "        plt.legend()\n",
    "        vert_line = 100\n",
    "        while vert_line < len(Y_test):\n",
    "            plt.axvline(x=vert_line)\n",
    "            vert_line = vert_line + 100\n",
    "        plt.title(key)\n",
    "        plt.show()\n",
    "    \n",
    "print(tabulate(list(print_results_1), headers=['Name', 'Avg. Acc', 'Precision', 'AUC', 'F1 Score', 'ErrP Acc', 'Non_ErrP Acc']))\n",
    "# print(\"\")\n",
    "print(tabulate(list(print_results_2), headers=['Name', '#Samples', 'Tr_split', 'Te_split', 'Train Avg. Acc', 'Train ErrP Acc', 'Train Non_ErrP Acc']))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_dict.keys()\n",
    "print(result_dict['data_S01'][1])\n",
    "test_acc = cmat_test.diagonal()*1.0/cmat_test.sum(axis=1)\n",
    "print(test_acc)\n",
    "test_prec = cmat_test[0,0]*1.0/(cmat_test[0,0] + cmat_test[1,0])\n",
    "print(test_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
